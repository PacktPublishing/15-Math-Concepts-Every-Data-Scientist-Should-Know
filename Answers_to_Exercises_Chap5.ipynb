{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8656a25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import loggamma\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2718084",
   "metadata": {},
   "source": [
    "## Q1: MAP estimation with user-supplied gradient function\n",
    "\n",
    "We're going to repeat the MAP estimation calculation in the main text, but with a user-supplied derivative function. The log-posterior is given by the following equation (hint: $x! = \\Gamma(x+1)$),\n",
    "\n",
    "$\\mathrm{log-posterior}\\;=\\;\\log \\Gamma(N+1) -\\log \\Gamma(N_{Head}+1) - \\log \\Gamma(N - N_{Head} + 1) + \\log \\Gamma(\\alpha + \\beta) - \\log \\Gamma(\\alpha) - \\log \\Gamma(\\beta) + (N_{Head} + \\alpha - 1)\\log p + (N-N_{Head} + \\beta - 1)\\log (1 - p)$\n",
    "\n",
    "So the gradient of the log-posterior, with respect to p is given by the following equation,\n",
    "\n",
    "$\\frac{\\partial \\mathrm{log-posterior}}{\\partial p} \\;=\\;\\frac{N_{Head} + \\alpha - 1}{p} - \\frac{N-N_{Head} + \\beta - 1}{1-p}$.\n",
    "\n",
    "So we need to generate functions that return the negative of the log-posterior and the negative of its gradient, both as a function of $\\log\\left( \\frac{p}{1-p}\\right )$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c75d5cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neg_log_binomial_posterior(n_trial, n_success, alpha, beta):\n",
    "    '''\n",
    "    A function to construct a callable that returns the negative of the \n",
    "    log-posterior for binomially distributed data, with a Beta \n",
    "    prior for the success probability of the Bernoulli trials. \n",
    "    \n",
    "    :param n_trial: The number of Bernoulli trials.\n",
    "    :type n_trial: int\n",
    "    \n",
    "    :param n_success: The number of successes.\n",
    "    :type n_success: int\n",
    "    \n",
    "    :param alpha: The alpha parameter of the Beta prior.\n",
    "    :type alpha: float\n",
    "    \n",
    "    :param beta: The beta parameter of the Beta prior.\n",
    "    :type beta: float\n",
    "    \n",
    "    :return: A callable that returns the negative of the log-posterior \n",
    "             (up to a global constant) and takes the logit of the success\n",
    "             probability logit(p) as input.\n",
    "    :rtype: A callable\n",
    "    '''\n",
    "    \n",
    "    def neg_log_binomial_posterior(logit_p):\n",
    "        '''\n",
    "        A function to compute the negative log-posterrior (up to a\n",
    "        global constant) for binomially distributed data, with a Beta \n",
    "        prior for the success probability of the Bernoulli trials. \n",
    "        \n",
    "        The function is a function of the logit of the success probability.\n",
    "        This is so the optimization algorithm can search between -infinity \n",
    "        and +infinity and still yield a valid success probability p that is \n",
    "        between 0 and 1. If we were to use p instead of logit(p) for our \n",
    "        parameter, we would have to perform a constrained optimization to \n",
    "        ensure we keep within [0,1]. The logit transformation is a \n",
    "        convenient way of doing this.\n",
    "        \n",
    "        We return the negative of the log-posterior because we will use \n",
    "        scipy's inbuilt minimization algorithms. Recall, maximizing a \n",
    "        function is the same as minimizing the negative of the function.\n",
    "        \n",
    "        :param logit_p: The logit of the success probability.\n",
    "        :type logit_p: float\n",
    "        \n",
    "        :return: The negative of the sum of the log-likelihood and \n",
    "                 the log-prior\n",
    "        :rtype: float\n",
    "        '''\n",
    "\n",
    "        # Compute the success probability p from logit(p)\n",
    "        p = np.exp(logit_p)/ (1.0 + np.exp(logit_p))\n",
    "        \n",
    "        # Compute the log-prior\n",
    "        log_prior = loggamma(alpha + beta) - loggamma(alpha) - loggamma(beta) \n",
    "        log_prior += ((alpha-1.0)*np.log(p)) + ((beta-1.0)*np.log(1.0 - p))\n",
    "  \n",
    "        # Compute the log-likelihood\n",
    "        log_likelihood = loggamma(n_trial + 1.0) - loggamma(n_trial - n_success + 1.0) - loggamma(n_success + 1.0)\n",
    "        log_likelihood += (n_success*np.log(p)) + ((n_trial-n_success)*np.log(1.0-p))\n",
    "  \n",
    "        # Compute the log-posterior, up to the global normalization factor,\n",
    "        # as the sum of the log prior and log-likelihood\n",
    "        log_posterior = log_likelihood + log_prior\n",
    "  \n",
    "        return -log_posterior\n",
    "\n",
    "    return neg_log_binomial_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f1a2148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_neg_log_binomial_posterior(n_trial, n_success, alpha, beta):\n",
    "    '''\n",
    "    A function to construct a callable that returns the gradient of negative of the \n",
    "    log-posterior for binomially distributed data, with a Beta \n",
    "    prior for the success probability of the Bernoulli trials. \n",
    "    \n",
    "    :param n_trial: The number of Bernoulli trials.\n",
    "    :type n_trial: int\n",
    "    \n",
    "    :param n_success: The number of successes.\n",
    "    :type n_success: int\n",
    "    \n",
    "    :param alpha: The alpha parameter of the Beta prior.\n",
    "    :type alpha: float\n",
    "    \n",
    "    :param beta: The beta parameter of the Beta prior.\n",
    "    :type beta: float\n",
    "    \n",
    "    :return: A callable that returns the gradient of the \n",
    "             negative of the log-posterior and takes the \n",
    "             logit of the success probability logit(p) as input.\n",
    "    :rtype: A callable\n",
    "    '''\n",
    "    \n",
    "    def grad_neg_log_binomial_posterior(logit_p):\n",
    "        '''\n",
    "        A function to compute the gradient of the negative \n",
    "        log-posterrior for binomially distributed data, with a Beta \n",
    "        prior for the success probability of the Bernoulli trials. \n",
    "        \n",
    "        :param logit_p: The logit of the success probability.\n",
    "        :type logit_p: float\n",
    "        \n",
    "        :return: The gradient of the negative of the sum of \n",
    "                 the log-likelihood and the log-prior\n",
    "        :rtype: float\n",
    "        '''\n",
    "\n",
    "        # Compute the success probability p from logit(p)\n",
    "        p = np.exp(logit_p)/ (1.0 + np.exp(logit_p))\n",
    "        \n",
    "        # Compute the gradient of the log-prior with respect to p\n",
    "        dlog_prior = ((alpha-1.0)/p) - ((beta-1.0)/(1.0 - p))\n",
    "  \n",
    "        # Compute the gradient of the log-likelihood with respect to p\n",
    "        dlog_likelihood = (n_success/p) - ((n_trial-n_success)/(1.0-p))\n",
    "  \n",
    "        # Compute the gradient of the log-posterior with respect to p\n",
    "        dlog_posterior = dlog_likelihood + dlog_prior\n",
    "        \n",
    "        # Compute the gradient of the log-posterior with respect to logit(p)\n",
    "        dlog_posterior_logit =  dlog_posterior * p * (1.0-p)\n",
    "  \n",
    "        return -dlog_posterior_logit\n",
    "\n",
    "    return grad_neg_log_binomial_posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d39038",
   "metadata": {},
   "source": [
    "Now we specify the data and generate the callable functions which return the negative log-posterior and its gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "263bdaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the sample size and the number of successes\n",
    "n_trial = 10\n",
    "n_success = 5\n",
    "\n",
    "# Specify the parameters of the prior\n",
    "alpha = 8\n",
    "beta = 2\n",
    "\n",
    "# Get the objective function to minimized\n",
    "neg_log_posterior = get_neg_log_binomial_posterior(n_trial, n_success, alpha, beta)\n",
    "\n",
    "# Get the gradient of the objective function to minimized\n",
    "grad_neg_log_posterior = get_grad_neg_log_binomial_posterior(n_trial, n_success, alpha, beta)\n",
    "\n",
    "# Construct an initial estimate for the optimal parameter.\n",
    "# We'll use the sample success proportion to do this (and take the logit)\n",
    "p0 = float(n_success)/float(n_trial)\n",
    "logit_p0 = np.log(p0/(1.0-p0))\n",
    "x0 = np.array([logit_p0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622d9449",
   "metadata": {},
   "source": [
    "Now minimize the negative log-posterior (equivalent to maximizing the log-posterior) using the scipy.optimize.minimize function and using the Broyden-Fletcher-Goldfarb-Shanno algorithm. We pass in the user-supplied gradient function using the 'jac' argument of the scipy.optimize.minimize function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1d193e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 1.651160\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n"
     ]
    }
   ],
   "source": [
    "map_estimate = minimize(neg_log_posterior, x0, method='BFGS', jac=grad_neg_log_posterior, options={'disp': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d39222",
   "metadata": {},
   "source": [
    "The algorithm has converged successfully using just a small number of function evaluations. Let's look at what is the MAP estimate for the value of $p$. It is very similar to the estimate we obtained when we just supplied the log-posterior function and the gradient calculationw were done internally and numerically by the scipy.optimize.minimize function, but in some cases the use of numerical gradient estimates can cause problems and so it is usually better to supply the exact gradient function where it is easy to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a891007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP estimate of success probability =  0.6666666672677467\n"
     ]
    }
   ],
   "source": [
    "# Convert from logit(p) to p \n",
    "p_optimal = np.exp(map_estimate['x'][0])/ (1.0 + np.exp(map_estimate['x'][0]))\n",
    "print(\"MAP estimate of success probability = \", p_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2018de84",
   "metadata": {},
   "source": [
    "## Q2 Maximum Likelihood estimate for a model of my coffee drinking habit\n",
    "\n",
    "The target variable here is a binary outcome $n_{i}$ which represents whether I drank coffee or tea on the $i^{th}$ morning of the dataset. $n_{i}=1$ means I drank coffee, whereas $n_{i}=0$ means I drank tea. We have $N=250$ observations in the dataset, so $i=1,2,\\ldots=250$. Also in the dataset is a variable $r_{i}$ which denotes whether it rained the night before. $r_{i}=1$ means it rained, where as $r_{i}=0$ means it didn't rain.\n",
    "\n",
    "Our model is a probabilistic one. It is a model of the probability, $p_{\\mathrm{coffee}}$, that I drink coffee in the morning, given the value of $r$. We use a very simple model for a linear predictor, and we will use a logit link function. So our model is,\n",
    "\n",
    "$\\mathrm{Prob}\\left( \\mathrm{Coffee} | r \\right ) \\;=\\; \\frac{\\exp \\left ( \\beta_{0} + r\\beta_{1} \\right )}{1 + \\exp \\left ( \\beta_{0} + r\\beta_{1} \\right )}$.\n",
    "\n",
    "Applying this model to calculate the log-likelihood of our dataset, we get the following formula for the log-likelihood,\n",
    "\n",
    "$\\mathrm{\\log-likelihood}\\;=\\;\\sum_{i=1}^{N}\\left [ n_{i}(\\beta_{0} + r_{i}\\beta_{1}) \\;-\\; \\log \\left ( 1 + \\exp ( \\beta_{0} + r_{i}\\beta_{1}) \\right ) \\right ]$.\n",
    "\n",
    "The gradient of this log-likelihood is given by the following formula,\n",
    "\n",
    "$\\frac{\\partial \\mathrm{log-likelihood}}{\\partial \\underline{\\beta}}\\;=\\;\\sum_{i=1}^{N}\\left [n_{i} -\\hat{p}_{i} \\right ]\\underline{x}_{i}$ ,\n",
    "\n",
    "where $x_{i} = (1, r_{i})$ and $\\hat{p}_{i} = \\exp(\\beta_{0} + r_{i}\\beta_{1}) / \\left [ 1 + \\exp(\\beta_{0} + r_{i}\\beta_{1}) \\right ]$.\n",
    "\n",
    "First, let's read in the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd0840f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the raw data\n",
    "df_coffee = pd.read_csv(\"./Data/coffee_or_tea.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1243a75f",
   "metadata": {},
   "source": [
    "Now we'll define user-supplied functions for the negative-log-likelihood and its gradient with respect to the parameters of the linear predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e80f575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neg_log_coffee_likelihood(df):\n",
    "    '''\n",
    "    A function to construct a callable that returns the negative of the \n",
    "    log-likelihood for a series of Bernoulli trials. \n",
    "    \n",
    "    :param df: A dataframe container the predictor features \n",
    "               and binary outcome variable. \n",
    "    :type df: A pandas dataframe\n",
    "    \n",
    "    :return: A callable that returns the negative of \n",
    "             the log-likelihood and takes the parameters \n",
    "             of the linear predictor as input.\n",
    "    :rtype: A callable\n",
    "    '''\n",
    "    \n",
    "    def neg_log_coffee_likelihood(beta):\n",
    "        '''\n",
    "        A function to compute the negative log-likelihood for \n",
    "        series of Bernoulli trials. \n",
    "        \n",
    "        The function is a function of the parameters of the linear predictor.\n",
    "        \n",
    "        We return the negative of the log-likelihood because we will use \n",
    "        scipy's inbuilt minimization algorithms. Recall, maximizing a \n",
    "        function is the same as minimizing the negative of the function.\n",
    "        \n",
    "        :param beta: The parameters of the linear predictor.\n",
    "        :type beta: A 1D numpy array \n",
    "        \n",
    "        :return: The negative of the log-likelihood\n",
    "        :rtype: float\n",
    "        '''\n",
    " \n",
    "        ## Compute the log-likelihood\n",
    "    \n",
    "        # Initialize the log-likelihood\n",
    "        log_likelihood = 0.0\n",
    "    \n",
    "        # Loop over the data points\n",
    "        for i in range(df.shape[0]):\n",
    "            # Calculate the linear predictor for the current observation\n",
    "            eta = beta[0] + beta[1]*df['rained'][i]\n",
    "            \n",
    "            # Update the log-likelihood\n",
    "            log_likelihood += (df['coffee'][i] * eta) - np.log(1.0 + np.exp(eta))\n",
    "  \n",
    "  \n",
    "        return -log_likelihood\n",
    "\n",
    "    return neg_log_coffee_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "064bb73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_neg_log_coffee_likelihood(df):\n",
    "    '''  \n",
    "    A function to construct a callable that returns the \n",
    "    negative of the gradient of the log-likelihood for \n",
    "    a series of Bernoulli trials. \n",
    "    \n",
    "    :param df: A dataframe container the predictor features \n",
    "               and binary outcome variable. \n",
    "    :type df: A pandas dataframe\n",
    "    \n",
    "    :return: A callable that returns the negative of \n",
    "             gradient of the log-likelihood \n",
    "             and takes the parameters of the linear \n",
    "             predictor as input.\n",
    "    :rtype: A callable\n",
    "    '''\n",
    "    \n",
    "    def grad_neg_log_coffee_likelihood(beta):\n",
    "        '''\n",
    "        A function to compute the gradient of the negative \n",
    "        log-likelihood for series of Bernoulli trials. \n",
    "        \n",
    "        The function is a function of the parameters of the linear predictor.\n",
    "                \n",
    "        :param beta: The parameters of the linear predictor.\n",
    "        :type beta: A 1D numpy array \n",
    "        \n",
    "        :return: The gradient of the negative of the log-likelihood\n",
    "        :rtype: float\n",
    "        '''\n",
    " \n",
    "        ## Compute the log-likelihood\n",
    "    \n",
    "        # Initialize the gradients\n",
    "        grad_log_likelihood = np.zeros(beta.shape[0])\n",
    "    \n",
    "        # Loop over the observations in the dataset\n",
    "        for i in range(df.shape[0]):\n",
    "            # Calculate the linear predictor for the current observation\n",
    "            eta = beta[0] + beta[1]*df['rained'][i]\n",
    "            \n",
    "            # Calculate the predicted probability of drinking coffee \n",
    "            # for the current observation\n",
    "            p_hat = np.exp(eta)/(1.0 + np.exp(eta))\n",
    "            \n",
    "            # Update the gradients\n",
    "            grad_log_likelihood[0] += (df['coffee'][i] - p_hat)\n",
    "            grad_log_likelihood[1] += df['rained'][i]*(df['coffee'][i] - p_hat)\n",
    "  \n",
    "  \n",
    "        return -grad_log_likelihood\n",
    "\n",
    "    return grad_neg_log_coffee_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20eb6af",
   "metadata": {},
   "source": [
    "Now we generate the callable functions which return the negative log-likelihood and its gradient. We'll also initialize the parameter estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab7994aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the objective function to minimized\n",
    "neg_log_coffee_likelihood = get_neg_log_coffee_likelihood(df_coffee)\n",
    "grad_neg_log_coffee_likelihood = get_grad_neg_log_coffee_likelihood(df_coffee)\n",
    "\n",
    "# Construct an initial estimate for the optimal parameters.\n",
    "# We'll keep it simple and initialize to beta_0 = 0.0 and beta_1 = 0.0\n",
    "# With this initialization there is an equal probability of drinking tea or coffee\n",
    "# for each of the observations.\n",
    "beta_init = np.array([0.0, 0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1b7fa3",
   "metadata": {},
   "source": [
    "Now let's minimize the negative log-likelihood (equivalent to maximizing the log-likelihood) using the scipy.optimize.minimize function and using the BFGS algorithm. We'll pass in our gradient function using the 'jac' argument of the scipy.optimize.minimize function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e40a7ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 124.913211\n",
      "         Iterations: 7\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n"
     ]
    }
   ],
   "source": [
    "max_like_estimate = minimize(neg_log_coffee_likelihood, beta_init, method='BFGS', jac=grad_neg_log_coffee_likelihood, options={'disp': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d71106",
   "metadata": {},
   "source": [
    "The minimization algorithm has converged. Let's look at the maximum likelihood estimates of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88789309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: Optimization terminated successfully.\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 124.91321062082822\n",
       "        x: [-3.655e-01  2.150e+00]\n",
       "      nit: 7\n",
       "      jac: [ 7.073e-07  8.879e-07]\n",
       " hess_inv: [[ 4.976e-02 -4.946e-02]\n",
       "            [-4.946e-02  9.638e-02]]\n",
       "     nfev: 9\n",
       "     njev: 9"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_like_estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5dec13",
   "metadata": {},
   "source": [
    "So we have maximum likelihood estimates of $\\hat{\\beta}_{0} = -0.3655$ and $\\hat{\\beta}_{1} = 2.150$. This means that when it hasn't rained the night before, the probability of me drinking coffee in the morning is $\\exp(-0.3655)/(1 + \\exp(-0.3655))\\;\\simeq\\;0.41$, i.e. about 41%. If it has rained, then that probability goes up to $\\exp(-0.3655 + 2.15) /(1 + \\exp(-0.3655 + 2.15)) \\;\\simeq\\; 0.86$, i.e. 86%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae215b2",
   "metadata": {},
   "source": [
    "## Q3 MAP estimate for a model of my coffee drinking habit\n",
    "\n",
    "We'll repeat the task of estimating $\\beta_{0}$ and $\\beta_{1}$ for my coffee probability model, but we'll now include a prior for each of the parameters and estimates $\\beta_{0}$ and $\\beta_{1}$ via MAP estimation. We'll use Gaussian priors for both $\\beta_{0}$ and $\\beta_{1}$, both with a mean of zero and a standard deviation of 1. This means, that in the absence of any data, the most probable model give a 50% chance of me drinking coffee in the morning, but the standard deviation of 1 means that we think there could be quite a wide range of variation in this prior probability of drinking coffee.\n",
    "\n",
    "For transparency, we'll just retype the same code when defininng functions that create the log-posterior and its gradient. We could have re-used the functions defined in Q2, but at the expense of making the code more opaque.\n",
    "\n",
    "First, we'll write code to generate functions that return the negative log-posterior and its gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9adbc79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neg_log_coffee_posterior(df, sigma_sq):\n",
    "    '''\n",
    "    A function to construct a callable that returns the negative of the \n",
    "    log-posterior for a series of Bernoulli trials. We have Gaussian priors, \n",
    "    with zero mean and user-defined variances.\n",
    "    \n",
    "    :param df: A dataframe container the predictor features \n",
    "               and binary outcome variable. \n",
    "    :type df: A pandas dataframe\n",
    "    \n",
    "    :param sigma_sq: The variances of the priors on the \n",
    "                     parameters of the linear predictor.\n",
    "    :type sigma_sq: A 1D numpy array\n",
    "    \n",
    "    :return: A callable that returns the negative of \n",
    "             the log-posterior and takes the parameters \n",
    "             of the linear predictor as input.\n",
    "    :rtype: A callable\n",
    "    '''    \n",
    "    \n",
    "    def neg_log_coffee_posterior(beta):\n",
    "        '''\n",
    "        A function to compute the negative log-posterior for \n",
    "        series of Bernoulli trials. \n",
    "        \n",
    "        The function is a function of the parameters of the linear predictor.\n",
    "        \n",
    "        We return the negative of the log-posterior because we will use \n",
    "        scipy's inbuilt minimization algorithms. Recall, maximizing a \n",
    "        function is the same as minimizing the negative of the function.\n",
    "        \n",
    "        :param beta: The parameters of the linear predictor.\n",
    "        :type beta: A 1D numpy array \n",
    "        \n",
    "        :return: The negative of the log-posterior\n",
    "        :rtype: float\n",
    "        '''        \n",
    "        \n",
    "        ## Compute the log-likelihood\n",
    "        log_likelihood = 0.0\n",
    "    \n",
    "        # Loop over the observations\n",
    "        for i in range(df.shape[0]):\n",
    "            # Compute the linear predictor for the current observation\n",
    "            eta = beta[0] + beta[1]*df['rained'][i]\n",
    "            \n",
    "            # Update the log-likelihood\n",
    "            log_likelihood += (df['coffee'][i] * eta) - np.log(1.0 + np.exp(eta))\n",
    "            \n",
    "        ## Compute the log-prior\n",
    "        log_prior = -0.5*np.log(2.0*np.pi*sigma_sq[0]) -0.5*np.log(2.0*np.pi*sigma_sq[1])\n",
    "        log_prior -= 0.5*np.power(beta[0], 2.0)/sigma_sq[0]\n",
    "        log_prior -= 0.5*np.power(beta[1], 2.0)/sigma_sq[1]\n",
    "        \n",
    "        ## Compute the log-posterior (up to a global constant)\n",
    "        log_posterior = log_likelihood + log_prior\n",
    "        \n",
    "        return -log_posterior\n",
    "    \n",
    "    return neg_log_coffee_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "420a0f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_neg_log_coffee_posterior(df, sigma_sq):\n",
    "    '''\n",
    "    A function to construct a callable that returns the negative of the \n",
    "    gradient of the log-posterior for a series of Bernoulli trials. \n",
    "    We have Gaussian priors, with zero mean and user-defined variances.\n",
    "    \n",
    "    :param df: A dataframe container the predictor features \n",
    "               and binary outcome variable. \n",
    "    :type df: A pandas dataframe\n",
    "    \n",
    "    :param sigma_sq: The variances of the priors on the \n",
    "                     parameters of the linear predictor.\n",
    "    :type sigma_sq: A 1D numpy array\n",
    "    \n",
    "    :return: A callable that returns the negative of the \n",
    "             gradient of the log-posterior and takes the \n",
    "             parameters of the linear predictor as input.\n",
    "    :rtype: A callable\n",
    "    '''      \n",
    "    \n",
    "    def grad_neg_log_coffee_posterior(beta):\n",
    "        '''\n",
    "        A function to compute the gradient of the negative \n",
    "        log-posterior for series of Bernoulli trials. \n",
    "        \n",
    "        The function is a function of the parameters of the linear predictor.\n",
    "                \n",
    "        :param beta: The parameters of the linear predictor.\n",
    "        :type beta: A 1D numpy array \n",
    "        \n",
    "        :return: The gradient of the negative of the log-posterior\n",
    "        :rtype: float\n",
    "        '''             \n",
    "        \n",
    "        ## Compute the gradient of the log-posterior\n",
    "        \n",
    "        # Initialize the gradients\n",
    "        grad_log_posterior = np.zeros(beta.shape[0])\n",
    "        \n",
    "        # Loop over the observations\n",
    "        for i in range(df.shape[0]):\n",
    "            # Compute the linear predictor for the current observation\n",
    "            eta = beta[0] + beta[1]*df['rained'][i]\n",
    "            \n",
    "            # Compute the predicted probability for the current observation\n",
    "            p_hat = np.exp(eta)/(1.0 + np.exp(eta))\n",
    "            \n",
    "            # Update the gradients\n",
    "            grad_log_posterior[0] += (df['coffee'][i] - p_hat)\n",
    "            grad_log_posterior[1] += df['rained'][i]*(df['coffee'][i] - p_hat)\n",
    "            \n",
    "        ## Add the gradients from the log-prior\n",
    "        grad_log_posterior[0] -= beta[0]/sigma_sq[0]\n",
    "        grad_log_posterior[1] -= beta[1]/sigma_sq[1]\n",
    "        \n",
    "        return -grad_log_posterior\n",
    "    \n",
    "    return grad_neg_log_coffee_posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a00a13a",
   "metadata": {},
   "source": [
    "Now we generate the callable functions which return the negative log-posterior and its gradient. We'll also initialize the parameter estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fbb5b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the variances for the priors\n",
    "sigma_sq = np.array([1.0, 1.0])\n",
    "\n",
    "# Get the objective function to minimized and its gradient function\n",
    "neg_log_coffee_posterior = get_neg_log_coffee_posterior(df_coffee, sigma_sq)\n",
    "grad_neg_log_coffee_posterior = get_grad_neg_log_coffee_posterior(df_coffee, sigma_sq)\n",
    "\n",
    "# Construct an initial estimate for the optimal parameters.\n",
    "# We'll keep it simple and initialize to beta_0 = 0.0 and beta_1 = 0.0\n",
    "# With this initialization there is an equal probability of drinking tea or coffee\n",
    "# for each of the observations.\n",
    "beta_init = np.array([0.0, 0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b152bae",
   "metadata": {},
   "source": [
    "Now let's minimize the negative log-posterior (equivalent to maximizing the log-posterior) using the scipy.optimize.minimize function and using the BFGS algorithm. We'll pass in our gradient function using the 'jac' argument of the scipy.optimize.minimize function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66b7bab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 128.892618\n",
      "         Iterations: 7\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n"
     ]
    }
   ],
   "source": [
    "map_estimate = minimize(neg_log_coffee_posterior, beta_init, method='BFGS', jac=grad_neg_log_coffee_posterior, options={'disp': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ad6b14",
   "metadata": {},
   "source": [
    "The minimization algorithm has converged. Let's look at the MAP estimates of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "519198f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: Optimization terminated successfully.\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 128.8926178531159\n",
       "        x: [-2.565e-01  1.949e+00]\n",
       "      nit: 7\n",
       "      jac: [ 3.943e-06  3.899e-06]\n",
       " hess_inv: [[ 4.470e-02 -4.282e-02]\n",
       "            [-4.282e-02  8.337e-02]]\n",
       "     nfev: 9\n",
       "     njev: 9"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf1b394",
   "metadata": {},
   "source": [
    "So we have maximum likelihood estimates of $\\hat{\\beta}_{0} = -0.2565$ and $\\hat{\\beta}_{1} = 1.949$. This means that when it hasn't rained the night before, the probability of me drinking coffee in the morning is $\\exp(-0.2565)/(1 + \\exp(-0.2565))\\;\\simeq\\;0.44$, i.e. about 44%. If it has rained, then that probability goes up to $\\exp(-0.2565 + 1.949) /(1 + \\exp(-0.2565 + 1.949)) \\;\\simeq\\; 0.84$, i.e. 84%. The prior, which prefers values closer to zero has moved both $\\beta_{0}$ and $\\beta_{1}$ closer (in magnitude) to zero in comparison to the maximum likelihood estimates. Consequently, the MAP estimates for the probabilities of drinking coffee, for both rain and not rain, are closer to 0.5 in comparison to the maximum likelihood estimate. The influence of the prior is clear. We don't whether the variance of 1.0 we have chosen for the priors is appropriate or not. In practice, we could either leave the variances of the priors as model parameters and estimate via MAP estimation, or we could even consider them to be hyperparameters that we tune via cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de836b",
   "metadata": {},
   "source": [
    "## Q4 MCMC sampling for a model of my coffee drinking habit\n",
    "\n",
    "We'll keep the log-posterior function we generated in the previous question, but use it to draw samples from the posterior using the Metropolis-Hastings algorithm. First, we'll make some changes to the MCMC code example in the main text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a699470d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_mh_trial(beta, log_posterior, delta_beta, neg_log_posterior):\n",
    "    '''\n",
    "    Function to perform a Metropolis-Hastings trial move\n",
    "    \n",
    "    :param beta: The current value of the linear predictor parameters\n",
    "    :type beta: float\n",
    "    \n",
    "    :param log_posterior: The current log-posterior value\n",
    "    :type log_posterior: float\n",
    "    \n",
    "    :param delta_beta: The half-widths of the range from which the trial \n",
    "                       adjustments to beta are made\n",
    "    :type delta_beta: float\n",
    "    \n",
    "    :param neg_log_posterior: A callable that returns the negative of the log-posterior \n",
    "                              (up to a global constant) and takes the parameters of the\n",
    "                              linear predictor as input.\n",
    "    :type neg_log_posterior: A callable    \n",
    "       \n",
    "    :return: A tuple of the updated current logit(p) value and \n",
    "             current log-posterior value\n",
    "    :rtype: A tuple of floats\n",
    "    '''\n",
    "    accept_trial = False\n",
    "    beta_trial = beta + (delta_beta*(2.0*np.random.rand(beta.shape[0]) - 1.0))\n",
    "    \n",
    "    # Calculate the log-posterior for the trial point.\n",
    "    # Note we'll need to flip the sign of neg_log_posterior, as \n",
    "    # our callable returns the negative of the log-posterior.\n",
    "    log_posterior_trial = -neg_log_posterior(beta_trial)\n",
    "    \n",
    "    # Calculate the change in log-posterior if we move to the trial point\n",
    "    delta_log_posterior = log_posterior_trial - log_posterior\n",
    "        \n",
    "    # Work out if should accept the trial point\n",
    "    if delta_log_posterior > 0.0:\n",
    "        accept_trial = True\n",
    "    else:\n",
    "        if np.log(np.random.rand(1)) < delta_log_posterior:\n",
    "            accept_trial = True\n",
    "            \n",
    "    # If we accept the trial point then update the current value of the parameter and \n",
    "    # the log-posterior\n",
    "    if accept_trial==True:\n",
    "        beta = beta_trial\n",
    "        log_posterior = log_posterior_trial\n",
    "        \n",
    "    return beta, log_posterior\n",
    "\n",
    "def mh_mcmc(n_burnin, n_iter, beta_init, delta_beta, neg_log_posterior):\n",
    "    '''\n",
    "    A function to run a simple Metropolis-Hastings MCMC\n",
    "    calculation.\n",
    "    \n",
    "    :param n_burnin: The number of burnin iterations to be run\n",
    "    :type n_burnin: int\n",
    "    \n",
    "    :param n_iter: The number of sampling iterations to be run\n",
    "    :type n_iter: int\n",
    "    \n",
    "    :param beta_init: The starting value for beta\n",
    "    :type beta_init: float\n",
    "\n",
    "    :param delta_beta: The half-widths of the range from which the trial \n",
    "                       adjustments to beta are made\n",
    "    :type delta_beta: float\n",
    "    \n",
    "    :param neg_log_posterior: A callable that returns the negative of the log-posterior \n",
    "                              (up to a global constant) and takes the parameters of the\n",
    "                              linear predictor as input.\n",
    "    :type neg_log_posterior: A callable\n",
    "    \n",
    "    :return: An array of the sampled beta values\n",
    "    :rtype: A 1D numpy array of length n_iter\n",
    "    '''\n",
    "    \n",
    "    #Calculate starting log_posterior\n",
    "    beta = beta_init\n",
    "    log_posterior = -neg_log_posterior(beta)\n",
    "    \n",
    "    # Run the chain for the specified burn-in length\n",
    "    for iter in range(n_burnin):\n",
    "        beta, log_posterior = perform_mh_trial(beta, log_posterior, delta_beta, neg_log_posterior)\n",
    "        \n",
    "    # Initialize an empty array to hold the sampled parameter values\n",
    "    beta_chain = np.zeros((n_iter, beta_init.shape[0]))\n",
    "    \n",
    "    # Continue the chain for the specified number of sampling points\n",
    "    # Store the sampled parameter values\n",
    "    for iter in range(n_iter):\n",
    "        beta, log_posterior = perform_mh_trial(beta, log_posterior, delta_beta, neg_log_posterior)\n",
    "        beta_chain[iter, :] = beta\n",
    "        \n",
    "    return beta_chain    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7b51ae",
   "metadata": {},
   "source": [
    "Now we'll run the MCMC calculation. We'll run a long burn-in period of 20000 iterations to be sure, and then we'll take 100000 samples. We'll start the chain from $\\beta_{0} = 0, \\beta_{1} = 0$. The trial step range will be 0.05 for both parameters. Running the MCMC chain can take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5dabb4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for the random number generator\n",
    "np.random.seed(1729)\n",
    "\n",
    "# Run the MCMC calculation. This can take several minutes to run.\n",
    "beta_chain = mh_mcmc(n_burnin=20000, \n",
    "                     n_iter=100000, \n",
    "                     beta_init=np.array([0.0, 0.0]), \n",
    "                     delta_beta=np.array([0.05, 0.05]), \n",
    "                     neg_log_posterior=neg_log_coffee_posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ec3e74",
   "metadata": {},
   "source": [
    "Finally, we plot histograms of the posterior sampled parameter values. We'll also overlay the MAP estimates so we can see how close the maximum of the histograms compare to the MAP estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de69ad59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcUAAAEzCAYAAABAChiAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA4sklEQVR4nO3deZgcVdn+8e+dkICIIBiQgMSoICCiLBGVNYARBLcXFFxAIioCyiLigiIG5BVFVlFWfxpQFBB8UZQdCWETTAQFBVwgIFtI2LdAEp7fH+c0qXR6q5nu6Z6Z+3NdffVU1anqp3uq+ulTdc4pRQRmZmYGI7odgJmZWa9wUjQzM8ucFM3MzDInRTMzs8xJ0czMLHNSNDMzy5wUzczMMidFMzOzbFAnRUnTJIWkyd2OxbpD0qskHSfpP5JezPvDrG7HZcPTYPlOGmzHjaQdJP1e0iOS5kt6VNJVkj7Y7tfqSlKUNDX/E6a1s2w/Y5osaYqkDTr5OtZ2vwG+BLwReB6YDcxpZcXCvhX5QFulSfkPF8o3/eKTtKykfSRdJOk+Sc9JelbSPZLOl7SbpFcMlrhsSOnzcTPQJJ0I/AHYEVgZeA5YCdgG+K2kj7bz9QZ1TRG4D7gLeLIN25oMfBvYoA3bsgEgaT3gPcB84N0RsUJErBoR7+jD5pYCPtGkzKdKxPYB4D/AycD7gTWAl4CFwHhgZ+DnwL8lbTMI47JBqs3HTUdJ+iywPynWQ4BXR8QKwFjgmlzs8+18zUGdFCPiUxGxTkT8X7djsa5YLz//LSL+1I/t3Jef6yYXSSuRfqk+AzzWaGO5pnYhsCrpR9vuwJiIWC4ilgdeDXwEmAasBmw5yOKywa1dx01HSVoKODxPfisivhcRTwJExMPAj/KyMe183UGdFG3Yq5zie6af27mRVHvaMP+KruVjwGjgAtLpppokvQ04lXRsXQxsGBG/iIhHK2Ui4smIuCAitgZ2BZ4eZHHZ4Nau46bTJpJ+nD0NnFBjeSUZ/redLzqok2K9i9qSRks6QNINkp7I12VmS/qrpB9Leneh7GRJAWyVZ/2s6vrMrBqvu5OkSyXNkfSCpPslnS1poybxjpR0oKS/SXo+r/97SZvl5ZXXHF+13qw8f6Kk1SWdLOnu/Nq3FsqtJGkPSRdIulPS0/la0T+ULqqv1iC24muMlXSqpP/mOO+Q9CVJIwrlPyrp2vz5PiXpD5Le2uj9N9Pq56p07TeAqXnWVlX/s4l9ePmf5+d6tbLK/LOabOd/gaWBB4BPRETdRAUQEecBxw3CuBZT5pjL5Xt2X63a/jhJP8nbn6d03fUYSSu0+tlUbfutkn6atzMvx3S9pL0ljWrHZ9vgtUsfN60ek4XyLX1XtajynXxFRLxQY/nH8vM1NZb1XUQM+IP0TwlgWn/Kkk7zBDC5MG+pwvwgXS95HFhQmHdOofyuwMPAi3nZk3m68vhzoewI4MzCdhbkbVemFwL71Hkfo0i/0Ctl5xfWnU+6llNZNr5q3Vl5/l6ki+EBPEv6pXdrodwxhW1U3kvxfT8CvK1OfJXX+DTwUJ31T8plv1d4/08Vlj8OrNWH/aHU5wocnP83T+blL1b9zzYtuR+eQ2pwEMD9wIiqcm/Oy+7Lsd5fvd/lcqvn/S2Ar7bh+OipuBrEW+qY6/V9tbD9z+Y4glRbeb6w/r+Asa18JxWWfTHvy5VtPFMV89XAsv39bBv8n1o+bujjdx0tfle1GG/l+/JrNZbtkZfNA1Zr6/7c7gOkxTc7lc4lxU8V/hG7Acvk+SOBccAXgEPK7MyFMl8v7JiHAq/K81cHzivsLFvWWPfwws51APCKPP/1wEVVO9z4Ojva08DfqnbeNQt/fwk4CtgQWK7wvjcGLs3buB1Qg535CeAG8hcSsGx+r5X3/Y18MB0AvDKXeStwZy5zXh/2hz59rqTGUS3tR032rXPy9HV5+j1V5Y7M84/K0/WSzycL/8N12nB89FRcDeItfcz18r5atf1/AZvn+SOAD7Hoy/7yVr9H8nqVRHgIsEqePwqYVIjptP5+ti38vybT5Lih78dk5bNr+F3VYpwP5m1NytNLAesCJ5EqEgF8ve37c7s32OKbnUrtXyq1HpVfZ0v8A2vtgKRWdQGcUjKmmjtzYfkrWfQL66gay0cC1+bl06uWLZcPhgC+UWPdUcCtNE+KjwOv7eNnvjTw97ydrWosr7zGY6QWXtXLryrEd1iN5Vuw6Jfb6BJx9edzbXpwt7gfVpLPXnn6rEIZFT6bdfO8esmnkqTmUePLfLDH1SDePh1zvbqvFrb/PDW+yIGtC9vfvGrZtOr/Qd6HK9v8nzrv+Q2k74j5FGqg7f5so4Xjpp/HZOV99vm7Km9nlcJnPAb4DIvXjhfQgbMeEdH1a4qjgNc2eSxTcptP5eexbYqx4r3A8qREfnT1wohYCHwnT24hadXC4u1IO9o84Ic11p1Pa9dvzoqI2SXjrrzGC8AVeXKzBkVPjYgnasy/Mj+/SO1Yrye9v6WBNUuE1p/Ptd3OI72HnSS9Ms/bilSbnxERdzRZ/zX5+fHIR/YQj6uircdcD+2r50XEv2vEdzWphgqptW4zE0n/q1lRp6V8RNwD/IlUG5pYWNSp77NG2nFM9vm7KtswP98fEXNJ+8HIwvKRwI6S1urHa9TU7aR4TUSo0YN0XruMS/LzhyT9Ll8ofk3DNVpTubD814h4vE6Z6aRfMMXysOgffGtE1GvxdW0LMdzYrICkdST9SKkxz1OSXqpcSCedRoLUoque2+rMfyQ/z6r1HiLiJWBunlyxWZwF/flc2yp/wV5E+gGzc57dakOWjunVuAr6dMwNgn11WoNllcYdreyPm+bn1SQ9XO/Boh8AaxTW7dT3WSPtOCabflc1UfnOvCU/f5b0v5pA6k/+BKnL0BWqGmhC0ihJRygNTDEv71/N+vq+rNtJse0i4hrgMNI/7AOkpupzc6u0Y/rxy2Ll/PxAg9eeB1SauK9cWFRpOvxQg+0/2EIMDUeckPQx0nn8LwDrs+g0yOz8eDYXfWXNDTSOcWGT5cUyNVvR1dGfz7UTKklm93yw7Uw6pfWrFtatxLiiJA2TuPp0zA2SfbXuPllY1sr+WKnljaa1s2LLVlbs4PdZI+04Jvs7Os5iSTEiXoqIJyJiZkQcQRrN5iVSDfw9VeueDnyT1Cd3P1KXjbMltTTIxZBLigAR8R1Sy7xDgMtIpyDWAb4M/KPVD6eOpfuwTitfRK2c1lpYb4GklYEzSAf5uaRfVMtExIqRRqtYFTi+RDwDrS+faydcSqppbENqLbg8cEk+hdNM5TTm0sDawyQuoNwxNwT2VSgXV+V79v+anRnLjynFlTv8fdZIf47Jut9VLdogP99Sa2FE3AJUTm2vUJmfu4pMBqZExP4RcQZp5KZpwA8kNX1PQzIpQjpHH2kEhO1J4+RtTaryLwWcrCbjSdZQ+eXz+noFJC3Dous3xV9Klb8bXRdodJqoFe8jNej5B6kf2sx8rbLotf18jU7oz+fadhGxgNQVYgSpbx8s6ivYzDUs+nHT1oGKezWuohLH3GDZVxsdk5VjuZX9sXJt7S19DaQD32eNdPWYzNfNK9d6/1KnzIjC6xdrtLuQapA/rszI19F/RGq8M7HZ6w/ZpFgUEQsjYhrpF8N80imZCVXFXsrP9X4BVv45a0lavU6ZLUk7abE8LPq1s4Gk5eqsu0Wd+a16XX7+W75msph82qwXx7Lsz+faKZVTlaNIreguamWliLif1LcKYD9Jy7eyXolTmr0aV63XbHTMDZZ9dasWlrWyP1aur62t+iMTtazF77P+6PYx+XZSbnosIu6rU2ZzUlJ8jtRAqWIj4D8RUT3k4U2F5Q0NuaQoaXSDxS+yqFpfXY2utPJ6dZ11L89lRgFfqfG6I4Fv5clrI43NV1z3WdI1gy/UWHcpUr+t/qgMiv7WOl9mnwPe1M/X6IT+fK4dEREzgSnAscCBUXs0jXoOBV4gffH/Mv+irkvSLsBBgzmuPhxzg2Vf3VXSG6tnStqSRY1ift3Cdq5i0Ti2x+d9uiZJK1ZN9/X7rD+6fUxWrie+ss7nP5LUzQjgl7H46EyrUfta8oOF5Q0NuaQInCXpZ5K2k/SqykylodPOJCWm51myteff8/NOqjGEU0Q8C3w3T+4v6ZuVWl/+NfUr0q+XSmfX4rpPs+gayZGS9qu0mJI0Djif1E+pP64knSJ7K/BDSa/O219e0ldIpxMerb96d/Tnc+1wXIdHxMERUap1Z0TcSvrhE6SBum9RuhXTSpUyklbIrQivJl1Te1XNjQ2euMoec4NlX30RuETSppBO2SndZeT8vPyKiLi+2UbyqeH9yB3RgcslvbPyg0DSUpI2lvQ94O6q1fv6fdZnPXBMVpLi0sClkjbPrz0i17R/Rzqz9hipEVLRK0g//haTz0jMZ9G4r/VFBzo/NnvQ2RFtLmRRB8/KkEjPFuYtAHavsa118ocZ+cN7gNQR9bpCmZEsOfTRYywaQmshsG+d9zGadJG8su6Led3K3/9TWDa2at1Zef7EJp/VcYVtVDo3Vzq8XsqiTtxTa6zb8DVobRSMluKssV6fPtdWYmpx32ppmKzCejU7yVeV+TDpWlLx//E0iw81Fvkzqx4VpCfjarDNCwvrtXTM9fK+Sv1h3p4rxNuXYd4+zaLvmCAls7ks3ik9+vvZtvD/auXz6esx2fB/02J8Mwrvt/hZFYfZewh4Z411byd19auePyKvd1Kz1x+KNcWvA18lHVh3k5LRSNLdBn4GbBQRSzRQiIg7Sb/iLiWd3lmVdKH5dYUyCyNiD1KH3ctJfWWWI/2DfgVsEhEn1woqIl4k/UL/MukfV7mH3UWk8/NXF4o/0Zc3HhEHkUY/uYV08C1FGinnwPzaC+qt2039+Vx7VURcSBq39Auk63n3k/4fS5G+OM4n3Sdx7YiYPsjjKn3MDZJ99d+ka3U/JX0nVEamORaYEBGNunwsISJ+Rmr9ewLpzNQCUsvJR0nH/8Gke1oW9en7rL+6dUzmS0mVwdp3AX5L+uEwirSf3Ew6dbteRNxUYxMPUbtBY+W0adOub8pZ1LpM0rak00r3RsT4LodjNmwp3Rnn9cDWkRq02ACRtD6p/+pzpPFWl2iI1WT975Gug64chcY2knYm/eDbPiIua7SNoVhTHKwqF7SvaFjKzGzoqlxPvL1sQsx+Tcpr+1Zm5Gu3XyR1HZnWbANLNStg7ZFbTJ0L/AS4MfIdpPOF48NJ46POp8bYqGZmw8TLQ2L2ZeWImCnp58DheZCI20jX0ScCe0YLLbadFAeOSENz7Qwg6SnS518Z0ukl4IsRUW88RzOzoW6D/PzXfmzjs8C9pAZFewP/JDVG+kUrK/ua4gDJVfi9STXC9UmjK4wi3R5rOnBCRAxEx3Qza8DXFLtH0uOkvuKbRwvdXToSg5OimZlZ0tXTp5IOAXYiNVN+gTRczyERcXuT9dYnjWW3CanvzGnAd6KQ4SVtReoLtR6pGe7REXFqs5jGjBkT48eP79P7MSvlrrvS89odGaPbbEDNnDlzbkR0+i42Hdfta4oTSXeW/jPpmtsRwJWS3hJLjl0HpFEvSC00pwPvICXUqaQOrcfmMm8g9cH6KbAbafSFkyXNiYgLGgU0fvx4ZsyY0e83ZtbUxInpedq0bkZh1haS7u12DO3Q1aQYEdsVpyXtTuokuxn1Bzv+JKlxyh6Rxry7XdK6wEGSjsu1xb2BByNiv7zOHZLeSeoc2zApmpnZ8NVr/RRfRYrp8QZl3k0ahLY4COxlpBELxhfKXF613mXABEllboBrZmbDSK8lxRNJ/VNubFBmVRbdn6xidmFZozJLAWOqNyhpL0kzJM2YM6ejt+szM7Me1jNJUdJxpGt/O0dEs7s2VzeZVY35rZRJMyJOj4gJETFh5ZUH/XViMzPro243tAFA0vHAx0j9gqpvnVLtYRbVCCsqd52e3aTMAnrjljRmZtaDul5TlHQiaVT+bfKdKpq5Edii6iapk0jdLmYVyrynar1JwIxI9zYzMzNbQleToqQfk+4x9nHgcUmr5sdyhTJHSbqqsNovSSOoT5X0Vkk7kW6vclyhn+KpwOsknSBpXUmfJQ35c8wAvC0zMxukul1T3JfU4vQq0n2wKo+DC2XGAm+qTOSBtCeRWpvOIN2l+1hSR/1KmXuAHUj3KbwV+Cawf7M+imZmNrx1u5+iWigzuca820gJr9F61wAb9Tk4MzMbdnqioY3ZcDf+639YYt6s7+3YhUjMhrdunz41MzPrGa4pmg2wSq3wnLtT76CP1aglmll3OCmadUitU6L9Xd+nVM06y6dPzczMMidFMzOzrOWkKOlESW/pZDBmZmbdVKamuB9wm6Tpkj4paXSngjIzM+uGMklxF9LIM5sBZwEPSjpG0todiczMzGyAtZwUI+L8iHgvaci1o4EXgYOAf0j6o6RdfANfMzMbzEp3yYiIWcAhkr4FfBjYC9gW2AqYK+lnwBkR8Z82xmlmuJuGWaf1uZ9iRCwAzgfOl7QJcAGwOvBV4GBJlwLfjoiZbYnUrIf1t09iu1/bidKsb/rVJUPSVpJ+CVxDSohzgBOA60h3qfiTpF37G6SZmdlAKF1TlLQSsAfptOmbAQHXA6cAv67cxDfXHn8DTAHObVO8ZmZmHdNyUpS0OfB5YGdgGeAZ4DTglHwrp8VExM35+uLX2hSrmZlZR5WpKU7Pz38n1QrPiohnmqzzQH6YmZn1vDLXFM8DtoqI9SPi5BYSIhFxakS8oVEZSVtK+p2kBySFpMlNyk/J5Wo9VsllJtZZvk6J92tmZsNMyzXFiPhYh2JYDridNCDAWS2UPwY4tWreOUBExCNV89cDHitMz+lrkGZmNvSVGft0Ye6b2KjMNyUtKBNARFwcEd+IiPOBl1oo/0xEPFx5AKOALYAzahR/pFg2IhaWic3MzIaXMqdPlR+tlBtInwGeIPWTrDZD0kOSrpK09cCGZWZmg027bx21IjCvzdusS9IIYE9So58XCoseAvYhtZTdCbgLuErSlnW2s5ekGZJmzJnjM6xmZsNVw2uKNZLI+DqJZSQwDvgkKQENlB2ANYCfFGdGxF1VcdwoaTxwMIta0RbLnw6cDjBhwoToVLBmZtbbmjW0mQZUkkSQOu3vUaesSNcEv9yWyFrzOeCGiPh7C2VvAjrVWMiGkW4O6dYqD/1m1jfNkuIRpGQo4DBSkrymRrmFwKPA1RFxZzsDrEfSasCOwGdbXGUD0mlVMzOzmhomxYiYUvlb0h7AhRHxw3YGIGk5YM08OQIYJ2kD4LGIuE/SUcAmEbFt1ap7As+S+k9Wb/NAYBZpoIHRwG6kO3rs3M7YzcxsaCnTT7FhJ/x+mABcXZg+PD/OBCYDY0n3cHyZJJFanZ4dEc/V2OZoUn/G1YHnSclxx4i4uN3Bm5nZ0NHnW0e1S0RMo0E3joiYXGNeAHWTdEQcTboRspmZWcvqJkVJfyQ3romI+/N0K6LGqU4zM7Oe16imOJGUFJctTLfCXRrMzGxQqpsUI2JEo2kzM7OhxonOzMwsa0tDG0krAi9GxLPt2J5ZrxgMHfXNrH3K3CVjW0lH5wRYmbeKpGuAucBjko7rRJBmZmYDoczp0/2AnSLi8cK8Y0i3bfo3aUSbAyTt0sb4zMzMBkyZpPh24LrKhKRXAB8BroiItYG1gf8Ce7c1QjMzswFSJimuAjxYmH4nsAwwFSAingZ+T0qOZmZmg06ZpPgC8IrC9BakPonFWzE9BazUhrjMzMwGXJmkeA+wTWF6Z+BfEfFAYd4apEY3ZmZmg06ZpHgmsL6kmyRdC6wP/LKqzEYM7E2GzczM2qZMP8VTgHcBu5IG8L4I+H5loaRNgHWBX7UzQDMzs4FS5tZR84FPSNo7TcbTVUXuBjYk3cfQzMxs0Ck9ok1EPFVn/lx8PdHMzAaxrt9P0axXeEg3MyuVFCVtBXwF2ARYkdoNdSIiWt6upC2Bg4GNgdWAT0fE1Ablx5NawlZ7X0RcWhXrccB6pP6VR0fEqa3GZTbU1Er6s763YxciMetdZZLXjsCFwEjgPlIr0wVtiGE54HbgrPxo1fbAXwvTj1X+kPQG4GLgp8BuwObAyZLmRMQF/Y7YzMyGpDI1xSnAfGDHiLi8XQFExMWkBIakqSVWfTQiHq6zbG/gwYjYL0/fIemdpBqpk6KZmdVUpp/iW4Fz25kQ++k3kh6RdL2kj1QtezdQHedlwARJo6o3JGkvSTMkzZgzZ06n4jUzsx5XJik+Q+EUZRc9Q6rx7QLsAFwFnCtpt0KZVYHZVevNJtWMx1RvMCJOj4gJETFh5ZVX7kzUZmbW88qcPr2KVAPrqtz149jCrBmSxgBfBX5RLFq1qurMNzMzA8rVFL8GvEnSoZLUtPTAuglYqzD9MKm2WLQKqWHQowMVlJmZDS5laorfBv4OHA7sKelW4Ika5SIiPtP/0ErZAHioMH0j8OGqMpOAGXlkHjMzsyWUSYqTC3+Pz49aAmg5KUpaDlgzT44AxknaAHgsIu6TdBSwSURsm8vvQWoFewvwEvAB4AukmmzFqcAXJZ0AnAZsluP/eKtxmZnZ8FMmKb6hQzFMAK4uTB+eH2eSEtlY4E1V6xwKvB5YCPwT2DMiXr6eGBH3SNoBOB7Yh9R5f3/3UTQzs0bKDAh+bycCiIhpLGoEU2v55KrpM0kJs9l2ryHdysrMzKwlZRramJmZDWmlk6KkD0g6R9JfJf27MH9dSV+VtHp7QzQzMxsYZcY+FTCVNJYowPPAKwpFHge+SzoV+n3MepjviGFmtZSpKe4L7A78DFgJOKa4MI9Dej3gYffNzGxQKpMUP0O6K8XnIuJJao8M8y8610rVzMyso8okxbWBqyOi0TBpjwAePNTMzAalMklxAbBMkzKrkwbsNjMzG3TKdN7/BzBRkmrVFiUtA2xDGmnGrGe4UY2ZtapMTfHnwDrA8ZIWW0/SSOA4YDVSC1UzM7NBp0xN8TTgg8D+wEeBpwEknQ+8i5QQfxsRZ7c7SDMzs4HQck0xIhYC7weOAEYDbyb1SdwJWBb4DilZmpmZDUplaopExAJgiqTDSUnxNcCTwJ05aZrZIFLreuus77mrsQ1fpZJiRW5oc1ebYzEzM+uqUklR0rrAJqS+iAHMAW6KCCdIMzMb9FpKipI2B04k3eG+1vK/kO5XeGP7QjMzMxtYTRvaSHofcAWwITCPNL7pecCv89/zgI2BP0qaVDYASVtK+p2kBySFpMlNyk+U9FtJD0l6TtLfJO1Zo0zUeKxTNj4zMxs+GtYUJb0S+H/AKFKr02Mi4pmqMssBXwW+AfxU0psj4vkSMSwH3A6clR/NbArcBhwNPARsB5wuaV5E/LKq7HrAY4XpOSXiMjOzYabZ6dOPAqsC34yIo2oVyEnyMEnzSN0yPkLq6N+SiLgYuBhA0tQWyn+3atYpkrYGdgaqk+IjETG31VjMzGx4a3b69H2kmtaxLWzrWNI9FXfob1B9sHx+7Woz8mnWq3LiNDMzq6tZTfFtwPSIeLHZhiLiBUnTgbe3JbIWSXo/sC2wWWH2Q8A+wJ9JAw3sDlwlaWJETK+xjb2AvQDGjRvX8ZjNzKw3NUuKryWf2mzRf4CJfY6mJEmbkU6Z7h8RN1fm5y4ixW4iN0oaDxwMLJEUI+J04HSACRMmNLo1lpmZDWHNTp++CniqxPaeITWc6bjcTeQS4LCIOKWFVW4C1upsVGZmNpg1qymOJHXSL2NkH2NpmaQtgT8AUyLihBZX24B0WtWGMN8mysz6o5XO++NzEmrF+LIB5C4da+bJEcA4SRsAj0XEfZKOAjaJiG1z+YmkhHgycLakVfO6CyNiTi5zIDAL+DvpmuJuwIdJLVTNzMxqaiUp7pEfrRDla5YTgKsL04fnx5nAZGAs8KbC8smku3IcnB8V97IoKY8GjgFWB54nJccdc/cPM2vAg4TbcNYsKU6nfJIrJSKmkZJpveWTa0xPrlW2UOZoUud+MzOzljVMihExcYDiMDMz67qWbzJsZmY21DkpmpmZZU6KZmZmmZOimZlZ5qRoZmaWOSmamZllTopmZmZZy0lR0iqdDMTMzKzbytQU/yvpXEnbdCwaMzOzLmpl7NOKfwIfBT4i6T/AacDUiHi0I5GZNeE7YphZu7VcU4yI9YHNgZ+TBtr+AXC/pLNL3EXDzMysZ5VqaBMRN+QBuVcDDgD+DXwcuFrSHZIOkLRi+8M0MzPrvD61Po2IJyPipELt8SxgHHAc8ICkqZImtDFOMzOzjmtHl4xHgceBeaRbQI0GPgXcJOlCSSu14TXMzMw6rkxDm5dJGkW6i/3ngS1JyfCfwHeAqcAGwFeBDwI/Jp1iNbNByjcetuGiVE1R0pqSjgYeAM4GNgMuBCZFxDoRcUJEPBER0yJiB+A3wPZNtrmlpN9JekBSSJrcQhzrS7pG0vN5vcMkqarMVpJmSpon6W5Je5d5r2ZmNvyU6bx/JXAXcDDwIqlWOD4ido6Iq+qsNhNYvsmmlwNuJzXceb6FOJYHrgBmA+8A9ge+AhxUKPMG4GLgBmBD4CjgJEk7N9u+mZkNX2VOn24DXA2cDFwYEQtbWOci4MFGBSLiYlICQ9LUFrb5SWBZYI+IeB64XdK6wEGSjouIAPYGHoyI/fI6d0h6JymhX9DCa5iZ2TBU5vTpuhGxbURc0GJCJCJuj4gz+xhbPe8Grs0JseIyUjeR8YUyl1etdxkwIV8PNTMzW0KZpLhrs076kraQdFg/Y2pmVdKp06LZhWWNyiwFjKneoKS9JM2QNGPOnDntjNXMzAaRMklxCjCxSZktgW/3NZgSompaNea3UibNiDg9IiZExISVV165TSGamdlg0+5bRy0FvNTmbVZ7mEU1worKHTxmNymzgNSv0szMbAntToobA3PbvM1qNwJbSFqmMG8SqUHPrEKZ91StNwmYERHzOxyfmZkNUg1bn0r6Y9WsyZIm1ig6ElgDeD3wqzIBSFoOWDNPjgDGSdoAeCwi7pN0FLBJRGyby/ySdIp2qqQjgTcDXwcOzy1PAU4FvijpBNLdPDYDJuNBBAYt3xHDzAZCsy4ZEwt/B6l15/ga5V4inZY8F/hSyRgmkLp6VByeH2eSEtlY4E0vBxHxpKRJpJFyZpCGmDuWNO5qpcw9knYAjgf2IdUi948Id8cwM7O6GibFiHj59Kqkl4ApEXFEOwOIiGksagRTa/nkGvNuIzXqabTda4CN+hmemZkNI2U6738auKVTgZiZmXVby0mxA53wzczMekrdpFjoqH9zRMxr1nG/KCKm9zsyMzOzAdaopjiN1LhmXdJtoSrTrRjZr6jMrOf5dlI2FDVKikeQkuDcqmkzM7MhqW5SjIgpjabNzMyGmnaPaGNmZjZoOSmamZlljVqfVg/x1qooDMlmZmY2aDRqaDOxj9t0YxwzMxuUGjW08alVMzMbVpz4zMzMMidFMzOzzMO8Wc/xvRPNrFs8zJuZmVnmYd7MzMyynhjmTdK+wFeAscDfgQMj4to6ZacA366zqddGxCOSJgJX11i+bkTc2e+AzcxsSCpzk+GOkLQrcCKwL3Bdfr5E0lsi4r4aqxwDnFo17xzSoAGPVM1fD3isMD2nPVGbmdlQ1KekKGkLYENgBeBJ4JZ6NbsWHARMjYgz8vR+krYH9gEOqS4cEc8AzxRiWQPYAti9xrYfiYi5NeabmZktoVRSlLQZ8FNgzcos8nVGSf8CPhMR15fY3mhgY1Ltr+hyYNMWN/MZ4AngghrLZkhaGvgHcGRE1DqlamZmBpRIipI2Bq4AlgGuIbVGfRhYFdga2BK4XNIWEfGXFjc7htRSdXbV/NnAe1qIaQSwJ3BWRLxQWPQQqab5Z2A0qRZ5laSJtbqLSNoL2Atg3LhxLYZuZtV842Eb7MrUFP83l/9QRFxUtexwSR8Czs/l3lcyjupWraoxr5YdgDWAnyy2sYi7gLsKs26UNB44GFgiKUbE6cDpABMmTHALWzOzYapMUtwU+E2NhAhARPxW0v8B25XY5lxgIam2WbQKS9Yea/kccENE/L2FsjcBHysRmw0Ad9Q3s15SZpi3l4B/NynzL0r0ZYyIF4GZwKSqRZOAGxqtK2k1YEfgjEblCjYgnVY1MzOrqUxNcQbw9iZl3g7cXDKG44CfS7oZuB7YG1iN3O1C0lHAJjXu0bgn8CxwXvUGJR0IzCL1eRwN7AZ8GNi5ZGxmZjaMlEmKhwLTJO0TEadUL5T0BWBbSt6HMSLOlfSavP2xwO3ADhFxby4yFnhT1WuJ1Or07Ih4rsZmR5NatK4OPE9KjjtGxMVlYjMzs+Gl0YDgh9WY/UfgR7kmdi3put9rgc2BtYBLgfeSrt+1LCJOBk6us2xyjXkBvKHB9o4Gji4Tg5mZWaOa4pQGy9bKj2rvA7YHvtOPmMzMzLqiUVLcesCiMDMz6wGNBgS/ZiADMTMz67YyXTLMzMyGNCdFMzOzrOyA4GNJXSe2I3V3GF2jWERE129JZWa9weOh2mBSZkDw1Ukd819L6ve3NHAv8ALwxrytW0m3kjIzMxt0ytToDiONUbpdRFwp6SXgZxFxhKTXkYZbG0/qwG+2BI9zama9rsw1xe2ASyPiyuoFEXE/8FHgFcDhbYrNzMxsQJVJiquSTptWLCQlQQAi4hnS/RY/1J7QzMzMBlaZpPgUizeseZzU2KboSWDl/gZlZmbWDWWS4r2kG/pW/BXYRtKyAJJGkMY9vb994ZmZmQ2cMknxKmBrSaPy9JmkWzzdIOkHpNs+rQec294QzczMBkaZ1qf/j3TKdAzwUET8QtLGwH7A23KZc4D/bW+IZmZmA6PlpBgR/wK+XzXvS5K+S+qnOCsiZrc5PjMzswHT75FnImIOMKcNsZiZmXVVn8Y+lbSGpA9K2j0/r9F8rYbb21fSPZLmSZopaYsGZcdLihqP7avKbZW3NU/S3ZL27k+MZmY29JUd+3Qt4GRgmxrL/gh8ISL+WXKbuwInAvsC1+XnSyS9JSLua7Dq9qQWsBWPFbb5BuBi4KfAbsDmwMmS5kTEBWXis/I8co2ZDVZlxj5dE7gBeA3wH1ICe5jUqX9z0vBu10naNCL+XSKGg4CpEXFGnt4v1/r2AQ5psN6jEfFwnWV7Aw9GxH55+g5J7wQOBpwUzcyspjI1xaNICfEA4McR8VJlQe6juB9wPPBdYJdWNihpNLAxcEzVosuBTZus/htJywD/Ao6PiPMLy96dt1F0GbCHpFERMb8qjr2AvQDGjRvXSuhm1g/1zib47hnWbWWuKW4LXBwRJxUTIkBEvBQRJwKXAO8psc0xwEigutXqbFINtJZnSDW+XYAdSP0nz5W0W6HMqnW2uVR+zcVExOkRMSEiJqy8sgfkMTMbrsrUFEeTbg3VyK3Aln2II6qmVWNeKhgxFzi2MGuGpDHAV4FfNNlmrflmZmZAuZriX4E1m5RZE/hbiW3OJQ0sXl0rXIUla3qN3ASsVZiuXOus3uYC4NES2zUzs2GkTFL8LrCTpPfVWihpR+B/KDGiTUS8CMwEJlUtmkRq1NOqDYCHCtM3suRp3EnAjOrriWZmZhV1T59K+lSN2ZcAv5d0FTCdVJt7LbAVqZvGRdS4ZtfEccDPJd1MGj91b9KYqqfmOI4CNomIbfP0HsB84BbgJeADwBeArxW2eSrwRUknAKcBmwGTgY+XjM3MzIaRRtcUp1L/utx7qN2g5oOkJHVWqwFExLmSXgMcCowFbgd2iIh7c5GxwJuqVjsUeD3p1Os/gT0j4uXriRFxj6QdSK1h9wEeBPZ3H0UzM2ukUVL89EAFEREnkwYFqLVsctX0maQ7dDTb5jXARu2Iz8zMhoe6STEnHzOzAVOr/6L7LtpA6veA4Da8eUg3MxtKSidFScsCOwEbAq8GngT+AvxfRDzb1ujMzMwGUNkBwXcgXc9biUWNbiA1yDle0qcj4vdtjM/MzGzAlBkQfCPgN6Rh2c4G/kjqGziW1B3j48D5kjaLiJkdiNXMzKyjytQUv0mqEW4REX+qWjZV0o+BacA3gJ3bE56ZmdnAKTOizRbAr2skRAAi4ibg/FzOzMxs0CmTFFcA/tukzH3A8n0Px8zMrHvKnD59ENikSZkJLD4GqZlZv7jvog2kMjXFi4FtJH1d0sjiAkkjJH2ZNPTbxe0M0MzMbKCUqSl+B/gw6S4Yn5d0LalWuCqwOTCedMumI9sbopmZ2cBoOSlGxMOSNifdgWISaUDuoiuAvSPCp0+HKI9eY2ZDXanO+xFxD7CdpNVJI9qsQBrR5paIeKAD8ZmZmQ2YMp337wYuiYgv5AToJGhmZkNKmYY2K5NqhWZmZkNSmaT4d5a82W9bSNpX0j2S5kmaKanuAACSJkr6raSHJD0n6W+S9qxRJmo81ulE/GZmNjSUuab4Q+Ankt4WEX9rVwCSdgVOBPYFrsvPl0h6S0TcV2OVTYHbgKNJrV+3A06XNC8ifllVdj3gscL0nHbFbWbd476L1illkuL9wJXA9ZJOA/5M6oIR1QUjYnqJ7R4ETI2IM/L0fpK2B/YBDqmx7e9WzTpF0tak8Vark+IjETG3RCxmZjaMlUmK00gJUKREtkQyLBjZYNnLJI0GNgaOqVp0OalG2KrlSUm72gxJSwP/AI6MiKtLbNPMzIaZMknxCBonwr4YQ0qgs6vmzyaNjtOUpPcD2wKbFWY/RKpp/hkYDewOXCVpYq1arKS9gL0Axo0bV/ItDE3uk2hmw1GZzvtTOhhHdbJVjXlLkLQZ6ZTp/hFx88sbi7gLuKtQ9EZJ44GDgSWSYkScDpwOMGHChHYnfjMbAL7OaO3QUlKUNA54BylR/Tkimt0to1VzgYWkoeKKVmHJ2mN1TJuTxlk9LCJOaeG1bgI+1pcgzcxseGjaJUPSMcDdwHnAr4F7JP2gHS8eES8CM0nDxhVNAm5oENOWwCXA4RFxQosvtwG+g4eZmTXQsKYo6RMsalRzJ+m05trAQZL+EhG/akMMxwE/l3QzcD2wN7AaaYxVJB0FbBIR2+bpicAfgJOBsyVVapkLI2JOLnMgMIvUt3I0sBtpMPOd2xCvmZkNUc1On34GWABsV2m5Kek9pFraZ4B+J8WIOFfSa4BDgbHA7cAOEXFvLjKWxQcNmAwsS7o+eHBh/r2kO3VASoTHAKsDz5OS444R4dtamZlZXc2S4tuAC4tdGSLiSkm/BSa2K4iIOJlU86u1bHKN6cm1yhbKHE3q3G9mZtayZklxRRZvxVlxJ+l0pA0B7n5hZpY0a2gzAphfY/580vVFMzOzIaOVAcHdb8/MzIaFVvopTpE0pdYCSQtrzI6IKHXzYhs4PlVqw4k79FtZrSSvsqdJfVrVzMwGpYZJMSLK3G/RzMxsUPNpziHMp0rNluRTqtaIa4JmZmaZa4pmNuy59mgVTopDhE+Vmpn1n5OimVkNrj0OT76maGZmlrmmaGbWItcehz4nxUHI1w/NzDrDSdHMrB9cexxafE3RzMws64maoqR9ga8AY4G/AwdGxLUNyq8P/AjYBHgMOA34TkREocxWwHHAesCDwNERcWrH3kSH+FSp2eDj2uPg1fWkKGlX4ERgX+C6/HyJpLdExH01yi8PXAFMB94BrA1MBZ4Fjs1l3gBcDPwU2A3YHDhZ0pyIuKDT76mvnADNzLqr60kROAiYGhFn5On9JG0P7AMcUqP8J4FlgT0i4nngdknrAgdJOi7XFvcGHoyI/fI6d0h6J3Aw0BNJ0QnQzKz3dDUpShoNbAwcU7XocmDTOqu9G7g2J8SKy4DvAOOBe3KZy6vWuwzYQ9KoiJjfz9DrcrIzs1p67bvBp3Nr63ZNcQwwEphdNX828J4666wK3F+jfGXZPfn5yhpllsqv+VBxgaS9gL3y5DOS7qqKcW7Dd9F9jrH/Bjy+d1f++P77W12l1z9D6P0Yez0+GKAY9f1+rV4rxtf3a4s9ottJsSKqplVjXrPy1fNbKZNmRJwOnF7rhSTNiIgJDWLpOsfYf70eHzjGduj1+MAxdlu3u2TMBRaSanZFq7Bk7bHi4TrlKaxTr8wC4NE+RWpmZkNeV5NiRLwIzAQmVS2aBNxQZ7UbgS0kLVNV/kFgVqFM9enXScCMTl5PNDOzwa3bNUVIfQknS/qspHUlnQisBpwKIOkoSVcVyv8SeA6YKumtknYCvg4cV+ineCrwOkkn5G1+FpjMkg16WlHztGqPcYz91+vxgWNsh16PDxxjV6nQ3717QaTO+18ldd6/HfhSREzPy6YCEyNifKH8+sCPSZ33HyclwSNqdN4/nkWd978/GDvvm5nZwOmJpGhmZtYLeuH0qZmZWU9wUjQzM8ucFKtI2kvS1ZKekBSSxre43s6S/iHphfz8Px2Kb2lJJ0maK+lZSb+T9LoW1jtA0p2Snpd0v6QfS1qux2JcXtIPJT2YP8d/S9qll2IsrP/xvH/8vlfik/Q5SddKeizvv1dL2ryNMe0r6R5J8yTNlLRFk/LrS7om73MPSDpMkhqtM5AxSpoo6beSHpL0nKS/Sdqzk/GVjbFqvbUkPS3pmV6KT8mB+fvlhfx5fq+TMXZURPhReAAHksZcPZDU0X98C+u8m9QH8pvAuvl5AfDODsR3Cqnh0CRgI2AacCswssE6nwBeAHYnDYW3DWnkn//Xoc+wLzGOAv4EXEIawH18fn5Hr8RYWPeNpFGVpgO/75X4gLOBLwIbkgbKP5U0UP5abYhnV2A+8Lm8j58EPAOMq1N+eVJ/4fOAtwI7A08DX+7E59XHGL8BHAlslv+n++Tj9hO9EmNhvdGk7mt/AJ7ppfhIPQj+CXwof44bAjt0KsZOP7oeQK8+gAm0nhTPBa6omncl8Ks2x7QC8CLwycK8NYCXgO0arPcj4JqqeYcDt3fgc+trjHsBdwOjB+B/26cYc7lRwE3AHqS7s7Q9KfYnvqrtKCem/doQ003AGVXz/gUcVaf8PsBTwCsK8w4FHiA38OvA51YqxjrbOA+4oIP7Xp9iJLWk/xmpa1knk2LZ//PaOYmu26mYBvrh06ftUW8A8nqDmvfVxqQv5ZdfKyL+C9zR5LWuAzaQ9C4ASeOAD5Jur9VufY3xw8D1wEmSHs6noKdIGtVDMQL8LzArIs7sQFwV/YmvaDSwDKnbUp9p0cD91ft4XwbuX410FqCt+hhjLcvTz8+rnr7GKGlH4P3A/p2Iq/A6fYnvQ6Qfs9tLulvSLElnSlqlTvme56TYHqtSe1Dz6qHm2vE6C1lyIN6GrxUR55BOFU2XNB+4F7gN+Fqb4+tzjKTTLh8lJYMdgW+RbgF2VK/EKOm9pNNLe3cgpqK+fobVjiSd+vpdP+NpNHB/vXjqHROVZe3WlxgXI+n9wLZ0rmN66RgljQXOAHaPiKc7FFdFXz7DN5IGAv8YqRa7O7AOcJGkQZlfBmXQZUk6MjeKaPSY2M+XKTuoeTvja/haSgMZfIt0A+eNgJ2AiaRTqC3pdIykffER4HMRMTPSzaAPA/ZptXFGJ2OUNIZ0unSPiOhTTWIAPsPiax0AfB7YKSKe6ku8NXRi4P5269NxKGkz0mhZ+0fEzZ0IrKBMjL8ATomIP3U2pMWUiW8EsDQpaU+PiGtJiXET0k3gB51euUtGp51A2rkaua8f2683AHm9Qc2rnUBr8b2L9EtuDDCn6rWmN1j3SNL1zZ/k6dskvRL4iaQjImJBD8T4EDA/IhYW5t1BuqF09ba6EeNbSSMuXVnI0SMAJC0A1ouIu+qsOxDxvSwnxCOB97XpC75TA/e3U19iBECphe7FwGERcUoHYqvoS4zbAFtJ+naeFjAi73P7RrrDTzfjewhYEBH/LMz7F6nB0jjSNcpBZVgkxYiYS2fvT3YjqZXgDwrzGg1qvphW45M0k3RRexLpVy1KzfTXbfJay5J29qKFLPrl3gsxXg98QtKIiHgpz3szaZzblv53HY7xz8D6VfOOBFYEvkBqzdvN+CrrHgQcQWr9d12z12pFRLyYY5oE/LqwaBJwQZ3VbgS+L2mZiJhXKF8cuL9t+hgjkrYkteicEhEntDuuNsRYvc99iNS6fRNSo6Vux3c9sJSkN0XEf/K8N5Jyy73tjG/AdLulT689SL+SNiB1Ywhghzy9UqHMVRRaY5EuQi8gdeVYJz/Pp3NdMh4g3QVkQ+Bqqprq14hvCqkl4MeAN5B28n/ToVZ2fYxxjRzjSaQWbduRuj38oFdirLGNqXS2S0bZz/ArpFaru+T9uPJYoQ3x7Jq3/VlScj6RdL3y9Xn5UcBVhfIrkGqL55Bq2Tvl/2+nu2SUiXEiqcvKD6o+r5V7JcYa60+m810yynyGI0hdRa7J++mG+e8/ASM6FWcnH10PoNcepAQSNR6TC2VmAVOr1vsIcGfeoe4gXcvpRHzLkBLHo6Ra1EXAGlVlFouP9Kvt26TTGs8D/wVOBlbslRjzvHeRakLPk2peR9ChLhp9jbFq+VQ6lxT78n+eVWffrfseSsa0b36NF/IX4ZZVn8WsqvLrk073ziOdZvs2HeqO0ZcY83Stz2tWr8RYY93JdDAp9vH/PJZUs3ya1C7gbOC1nYyxkw8PCG5mZpYNi9anZmZmrXBSNDMzy5wUzczMMidFMzOzzEnRzMwsc1I0MzPLnBTNzMwyJ0UzM7PMSdGsx0haWtIhkm6T9Hx+3CrpM92OzWyo84g2Zj0k3+j1CmBL0lin04BXAB8n3QB3+4i4rFvxmQ11w+IuGWaDyAGkhHgasE/kX62SppPGlNycdAd7M+sAnz416y17kwYA/3Isfhqncs/LRwc+JLPhw0nRrEdIej3pXnRXRcSzVYt3yc9/HNiozIYXnz416x0T8vPLdyuXJGB/YGfgyoj4WzcCMxsunBTNesfG+XmmpK1JN7regnTT5b8Cu3UrMLPhwknRrHdUkuIM4Eeku6BX3AmMHPCIzIYZX1M06x0bAfdGxFzgk8AYYCJwASlBXlosLGlfSfdImidppqQtBjpgs6HGSdGsB+RGNmNItUQiYmFEPBoR10TER0inT9eX9MZcflfgROC7wIbADcAlksZ15Q2YDRFOima9oXjqtJbH8/PT+fkgYGpEnBERd0TEfsBDwD4djNFsyHNSNOsNLzeyqV4gaSVgM+C2iJiTR73ZGLi8qujlwKYdjdJsiHNSNOsNlaS4a+6GAbw87NtpwCjg2Dx7DKnRzeyqbcwGVu1wnGZDmlufmvWGSlL8DLCBpD8CrwLeS+rQPzUizqxap3rgYtWYZ2YlOCmadVluHDOGNBD4E8A2pGuGTwJ/Ab4eEb8urDIXWMiStcJVWLL2aGYlOCmadV+llnhFRPygWeGIeFHSTGASUEyWk0jdN8ysj5wUzbqvkhT/UmKd44CfS7oZuJ40kPhqwKltjs1sWHFSNOu+SlK8pdUVIuJcSa8BDgXGArcDO0TEvR2Iz2zY8E2GzbpM0iPAcxExvtuxmA13TopmZmaZ+ymamZllTopmZmaZk6KZmVnmpGhmZpY5KZqZmWVOimZmZpmTopmZWfb/AdOFVTuKFGDxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the histogram of posterior sampled beta_0 parameter\n",
    "# The red line is the MAP estimate value\n",
    "hist = plt.hist(beta_chain[:, 0], bins=60, density=True)\n",
    "plt.axvline(x = map_estimate['x'][0], color = 'r')\n",
    "plt.title(r'Histogram of MCMC samples for $\\beta_{0}$', fontsize=24)\n",
    "plt.xlabel(r'$\\beta_{0}$', fontsize=20)\n",
    "plt.ylabel('Probability Density', fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68019f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the histogram of posterior sampled beta_1 parameter\n",
    "# The red line is the MAP estimate value\n",
    "hist = plt.hist(beta_chain[:, 1], bins=60, density=True)\n",
    "plt.axvline(x = map_estimate['x'][1], color = 'r')\n",
    "plt.title(r'Histogram of MCMC samples for $\\beta_{1}$', fontsize=24)\n",
    "plt.xlabel(r'$\\beta_{1}$', fontsize=20)\n",
    "plt.ylabel('Probability Density', fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bc36b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from statsmodels.stats.weightstats import ttest_ind as ttest_ind_sm\n",
    "from statsmodels.stats.weightstats import DescrStatsW, CompareMeans\n",
    "from statsmodels.stats.power import TTestIndPower\n",
    "from scipy.stats import ttest_ind, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19c2cd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll first set the seed for the numpy random number generator\n",
    "np.random.seed(1869)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0ccb49",
   "metadata": {},
   "source": [
    "## Simple t-test\n",
    "\n",
    "We will use the t-test implementation available in scipy.stats. First we'll read in the data in the file hypothesis_test_example.csv in the Data directory of the repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c68f0220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in t-test example dataset\n",
    "df_simple_example = pd.read_csv(\"../Data/hypothesis_test_example.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71591a24",
   "metadata": {},
   "source": [
    "Let's look at what the data consists of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a36ed2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_A</th>\n",
       "      <th>x_B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.035733</td>\n",
       "      <td>-0.171019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.105017</td>\n",
       "      <td>0.655910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.745565</td>\n",
       "      <td>0.994498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.882300</td>\n",
       "      <td>1.146082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.369233</td>\n",
       "      <td>0.317476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.806429</td>\n",
       "      <td>1.124682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.320897</td>\n",
       "      <td>-0.413721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.910782</td>\n",
       "      <td>0.326461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.141452</td>\n",
       "      <td>0.309490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.803123</td>\n",
       "      <td>0.538578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        x_A       x_B\n",
       "0  2.035733 -0.171019\n",
       "1  1.105017  0.655910\n",
       "2 -0.745565  0.994498\n",
       "3  1.882300  1.146082\n",
       "4 -0.369233  0.317476\n",
       "5  0.806429  1.124682\n",
       "6  0.320897 -0.413721\n",
       "7  0.910782  0.326461\n",
       "8  1.141452  0.309490\n",
       "9  1.803123  0.538578"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_simple_example.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea57967e",
   "metadata": {},
   "source": [
    "Okay, we've got two simple columns of data. One column from the A group and one column of data from B group.\n",
    "\n",
    "Let's take quick look at the summary statistics of the data. We'll just use the describe function from pandas to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fab0fc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_A</th>\n",
       "      <th>x_B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.529000</td>\n",
       "      <td>0.176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.938470</td>\n",
       "      <td>0.983872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.036634</td>\n",
       "      <td>-1.957430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.241617</td>\n",
       "      <td>-0.448830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.544784</td>\n",
       "      <td>0.250698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.108813</td>\n",
       "      <td>0.676324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.126451</td>\n",
       "      <td>2.612088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              x_A         x_B\n",
       "count  100.000000  100.000000\n",
       "mean     0.529000    0.176000\n",
       "std      0.938470    0.983872\n",
       "min     -1.036634   -1.957430\n",
       "25%     -0.241617   -0.448830\n",
       "50%      0.544784    0.250698\n",
       "75%      1.108813    0.676324\n",
       "max      3.126451    2.612088"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract summary statistics of the data\n",
    "df_simple_example.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c08d8",
   "metadata": {},
   "source": [
    "From the summary statistics we can see there is difference in the sample means, with the B group sample data having a mean of 0.176, whilst the A group sample data has a mean of 0.529. But does this provide evidence for the underlying population means of group A and group B being different? Let's run the t-test to test this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db3fcc3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TtestResult(statistic=2.5961983095998966, pvalue=0.010132851609223453, df=198.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The scipy t-test function is really easy to use. We just pass in the two columns of data. We are assuming that \n",
    "# the underlying population variances are the same in each group. \n",
    "ttest_ind(df_simple_example['x_A'], df_simple_example['x_B'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd0438c",
   "metadata": {},
   "source": [
    "From this t-test we get a test statistic (t-value) of 2.596 and a p-value of 0.0101. If we use a $\\alpha$ threshold of $\\alpha=0.05$, then we would reject the null hypothesis and conclude that there is evidence (not proof) that the underlying population means of the two groups are different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d23318",
   "metadata": {},
   "source": [
    "For completeness, let's see how we can also run that t-test using the statsmodels package. We'll use the statsmodels.stats.weightstats.ttest_ind function. To avoid the clash in function names with the scipy.stats.ttest_ind function we have imported this as ttest_ind_sm. Again the function is very easy to use. We just pass the two columns of data to the function. The default settings will assume that the underlying population variances are the same for each group and construct an estimate of that common variance from all the data pooled together. The function will also assume we are doing a two-tailed hypothesis test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "231b1f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.596198309599896, 0.01013285160922347, 198.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the statsmodels version of the two-sample two-tailed t-test\n",
    "ttest_ind_sm(df_simple_example['x_A'], df_simple_example['x_B'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432eb3b0",
   "metadata": {},
   "source": [
    "The calculated t-value and p-value are the same as for the scipy version of the t-test. The 198 refers to the number of degrees of freedom used in the calculation of p-value. In this case the number of degrees of freedom is 200 - 2 = 198 because we have 200 observations in total but we have estimated two sample variances from that data when calculating the t-value.\n",
    "\n",
    "We can check that the p-value calculated by statsmodels and scipy is the same as from the t-distribution formula given in the main text with $\\nu=198$ by using the t-distribution implementation in scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a87b997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value estimate from right-hand tail of PDF =  0.010132851609223614\n",
      "p-value estimate from left-hand tail of PDF =  0.01013285160922349\n"
     ]
    }
   ],
   "source": [
    "# Calculate the p-value for the observed t-value of \n",
    "# t=2.5961983095998953 and and df=198 by using the cumulative \n",
    "# distribution function (CDF) of Student's t-distribution.\n",
    "# scipy gives us an implementation of the t-distribution. \n",
    "# We want to calculate the area under the PDF that is to the\n",
    "# right of 2.5961983095998953 and to the left of -2.5961983095998953.\n",
    "# Since the t-distribution is symmetric about zero this is calculated as\n",
    "# 2*(1 - CDF(2.5961983095998953)) or 2*CDF(-2.5961983095998953).\n",
    "\n",
    "p_value1 = 2.0*(1.0 - t.cdf(2.5961983095998953, 198))\n",
    "p_value2 = 2.0*t.cdf(-2.5961983095998953, 198)\n",
    "\n",
    "print(\"p-value estimate from right-hand tail of PDF = \", p_value1)\n",
    "print(\"p-value estimate from left-hand tail of PDF = \", p_value2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81ff7ec",
   "metadata": {},
   "source": [
    "Up to machine precision the two different ways of calculating the p-value from the CDF of the t-distribution implementation in scipy come out to be the same, and are the same as we get from the scipy and statsmodel t-test functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63984557",
   "metadata": {},
   "source": [
    "## Permutation based test of difference in means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c222e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed t-value is =  2.5961983095998966\n",
      "Permutation estimated p-value =  0.01032989670103299\n"
     ]
    }
   ],
   "source": [
    "# Set a large, but reasonable number of permutations to run. \n",
    "# In this case I've chosen to generate 100000 permuted datasets.\n",
    "# This may take a couple of minutes to run.\n",
    "n_permutations = 100000\n",
    "\n",
    "# First I'll combine the original data into a single array. This makes performing the permutation easier.\n",
    "x_All = np.concatenate((df_simple_example['x_A'].to_numpy(), df_simple_example['x_B'].to_numpy()))\n",
    "\n",
    "## Next I'll calculate the observed test-statistic value and store it \n",
    "## in a variable called t_observed\n",
    "\n",
    "# Create arrays to hold the indices of the datapoints \n",
    "# belonging to the A group and the B group. To start, the\n",
    "# A group datapoints are at indices 0:99. The B group datapoints\n",
    "# are at indices 100:199\n",
    "nA = df_simple_example.shape[0]\n",
    "nB = nA\n",
    "\n",
    "A_indices = np.arange(0, nA)\n",
    "B_indices = np.arange(nA, (nA+nB))\n",
    "\n",
    "# Calculate the mean of each sample group\n",
    "m_A = np.mean(x_All[A_indices])\n",
    "m_B = np.mean(x_All[B_indices])\n",
    "    \n",
    "# Calculate the sample variances of each sample group\n",
    "# The ddof=1 means we are using unbiased estimators for \n",
    "# the sample variance calculations\n",
    "s2_A = np.var(x_All[A_indices], ddof=1)\n",
    "s2_B = np.var(x_All[B_indices], ddof=1)\n",
    "    \n",
    "# Calculate the t-value test-statistic for the original data\n",
    "sigma2_observed = (((nA-1)*s2_A) + ((nB-1)*s2_B))/(nA+nB-2)\n",
    "t_observed = (m_A - m_B)/ (np.sqrt(sigma2_observed) * np.sqrt(2.0/nA))\n",
    "\n",
    "print(\"Observed t-value is = \", t_observed)\n",
    "\n",
    "## Now perform the permutations\n",
    "\n",
    "# Set our p-value estimate count to zero\n",
    "p_count = 0.0\n",
    "\n",
    "# Loop over the permutations\n",
    "for i in range(n_permutations):\n",
    "    #Generate the permutation\n",
    "    permuted_indices = np.random.permutation(nA+nB)\n",
    "    A_indices = permuted_indices[0:nA]\n",
    "    B_indices = permuted_indices[nA:(nA+nB)]\n",
    "    \n",
    "    # Calculate the mean of each sample group \n",
    "    # for the permuted dataset\n",
    "    m_A = np.mean(x_All[A_indices])\n",
    "    m_B = np.mean(x_All[B_indices])\n",
    "    \n",
    "    # Calculate the sample variances of each sample group\n",
    "    # for the permuted dataset\n",
    "    s2_A = np.var(x_All[A_indices], ddof=1)\n",
    "    s2_B = np.var(x_All[B_indices], ddof=1)\n",
    "    \n",
    "    # Calculate the t-value for the permuted dataset\n",
    "    sigma2_permuted = (((nA-1)*s2_A) + ((nB-1)*s2_B))/(nA+nB-2)\n",
    "    t_permuted = (m_A - m_B)/ (np.sqrt(sigma2_permuted) * np.sqrt(2.0/nA))\n",
    "    \n",
    "    # Update our count if the t-value for the permuted dataset \n",
    "    # exceeds (in magnitude) that for the real dataset\n",
    "    if np.abs(t_permuted) >= np.abs(t_observed):\n",
    "        p_count += 1.0\n",
    "        \n",
    "# Now estimate the p-value\n",
    "p_value_permutation = (1.0+p_count)/(1.0+n_permutations)\n",
    "print(\"Permutation estimated p-value = \", p_value_permutation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39fe257",
   "metadata": {},
   "source": [
    "The scipy.stats t-test implementation also allows us to estimate the p-value using a permutation-based calculation. So let's run the scipy.stats version and see how it compares. To do so we just specify the permutation argument to the scipy.stats.ttest_ind function. We'll use the same number of permutations as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f19e9689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TtestResult(statistic=2.5961983095998966, pvalue=0.00988990110098899, df=nan)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the scipy t-test with permutation-based p-value estimation\n",
    "ttest_ind(df_simple_example['x_A'].to_numpy(), df_simple_example['x_B'].to_numpy(), permutations=n_permutations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7b6f56",
   "metadata": {},
   "source": [
    "The permutation-based p-value estimates from our own code and from the scipy.stats.ttest_ind function are very similar. Obviously, since both are based on random generation of permutations we would expect to see differences, as the two different calculations will be generating different sets of permutations. As we increase the number of permutations used, we would expect the differences in the p-values estimates between the two methods to decrease. Try increasing n_permutations to 1000000 and re-running - but remember increasing n_permutations means the code takes 10 times longer to run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c352ee41",
   "metadata": {},
   "source": [
    "## Confidence Interval calculation\n",
    "\n",
    "We'll use the simple example data again to demonstrate how to use the statsmodels.stats.weightstats.CompareMeans class to calculate a confidence interval for the difference between two population means given two i.i.d. samples of data from those populations. We'll have to wrap the pandas series holding the samples as statsmodels.stats.weightstats.DescrStatsW objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02fd3573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.08486864535681571, 0.6211313546431838)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we'll instantiate a CompareMeans object to run\n",
    "# the confidence interval calculation. We just pass in\n",
    "# our two samples of data. We must wrap the samples as\n",
    "# DescrStatsW objects. Since we are not applying any \n",
    "# non-uniform weights to the observations we can just \n",
    "# pass the pandas series for each sample into the \n",
    "# constructor for the DescrStatsW class\n",
    "mean_comparison = CompareMeans(DescrStatsW(df_simple_example['x_A']), DescrStatsW(df_simple_example['x_B']))\n",
    "\n",
    "# Now compute the 95% confidence level for the \n",
    "# difference in means using the tconfint_diff method \n",
    "# of the CompareMeans class. The 95% confidence level \n",
    "# is the default\n",
    "mean_difference_95CI = mean_comparison.tconfint_diff()\n",
    "\n",
    "mean_difference_95CI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4700d07b",
   "metadata": {},
   "source": [
    "We can see that the 95% confidence interval does not cross zero, so we would conclude that a hypothesis test would reject a null hypothesis of their being no difference in population means, when tested at the $\\alpha=0.05$ level.\n",
    "\n",
    "Let's run another confidence interval calculation, but this time with a higher level of confidence, say 99%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f499d9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.0006375498458349727, 0.7066375498458345)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_difference_99CI = mean_comparison.tconfint_diff(alpha=0.01)\n",
    "\n",
    "mean_difference_99CI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea5c7a1",
   "metadata": {},
   "source": [
    "Clearly, the 99% confidence interval is wider than the 95% confidence interval. The 99% confidence interval straddles zero, so we would reject the null hypothesis at $\\alpha=0.01$ in a two-tailed test of the hypothesis $\\mu_{A} = \\mu_{B}$. The illustrates the subjective nature of hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa42555",
   "metadata": {},
   "source": [
    "## Power calculation for t-test\n",
    "\n",
    "We will use the statsmodels two-sample t-test power calculation function solve_power from the statsmodels.stats.power.TTestIndPower class. Unfortunately, this function may throw a warning due to warnings from the underlying Boost library that scipy makes use of. See the statsmodels issue https://github.com/statsmodels/statsmodels/issues/8624 for more details. For convenience of output we have suppressed the warning.\n",
    "\n",
    "The solve_power function takes several arguments, i) effect_size, ii) nobs1, iii) alpha, iv) power, v) ratio, vi) alternative. We specify all but one of effect_size, nobs1, alpha, power, ratio and the function will determine the value of the unspecified argument that is necessary for conistency with the other arguments. In the example below, we have left nobs1 unspecified, so the function will determine the number of observations needed to give us a power of 0.8, when the effect size $|\\mu_{A} - \\mu_{B}|/\\sigma = 0.5$, and the type-I error rate $\\alpha=0.05$. The argument ratio (set to its default value of 1.0 here) is the ratio of the sample sizes drawn from the A and B populations, so a ratio of 1.0 means we are calculating the value of $N$ required if we are going to have an equal number of observations from A and B. The alternative argument is the type of alternative hypothesis we want to test. 'two-sided' means \"two-tailed\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "378fb1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size required =  63.765611775409525\n"
     ]
    }
   ],
   "source": [
    "# Import the warnings module so we can ignore the warning thrown by scipy\n",
    "import warnings\n",
    "\n",
    "# Wrap the call to tt_ind_solve_power\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    print(\"Sample size required = \", TTestIndPower().solve_power(effect_size=0.5, nobs1=None, alpha=0.05, power=0.8, ratio=1.0, alternative='two-sided'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3174c3a0",
   "metadata": {},
   "source": [
    "To double check that this does give the required power, we can use the power function from the TTestIndPower class to calculate the power for the sample size we have just estimated. If the estimated sample size is correct, we should get back a power of 0.8. All the other arguments to the power function are the same as before, i.e. as we passed to the solve_power function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "592f0a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power =  0.7999999950676624\n"
     ]
    }
   ],
   "source": [
    "print(\"Power = \", TTestIndPower().power(effect_size=0.5, nobs1=63.765610587854034, alpha=0.05, ratio=1.0, alternative='two-sided'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5819d745",
   "metadata": {},
   "source": [
    "Note that if we calculate the power for a larger sample size, we should get a higher value. Let's try with a sample size of 85."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94e701a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power =  0.8998940700985045\n"
     ]
    }
   ],
   "source": [
    "print(\"Power = \", TTestIndPower().power(effect_size=0.5, nobs1=85, alpha=0.05, ratio=1.0, alternative='two-sided'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f566b654",
   "metadata": {},
   "source": [
    "We can see that the power has increased to nearly 90%, i.e. there is a 90% probability of rejecting the null hypothesis in a two-tailed t-test when $|\\mu_{A} - \\mu_{B}|/\\sigma = 0.5$, $\\alpha=0.05$ and $N_{A} = N_{B} = 85$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

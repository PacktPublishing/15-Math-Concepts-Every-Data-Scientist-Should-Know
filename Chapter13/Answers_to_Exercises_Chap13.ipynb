{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8a5cbcd",
   "metadata": {},
   "source": [
    "Below we give a reminder of the exercise questions and also answers to those questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba943c4f",
   "metadata": {},
   "source": [
    "# The exercise questions\n",
    "\n",
    "We will use a simple but instructive example. Our random variable $X$ will consist of three binary random variables $A_{1},A_{2},A_{3}$. So we denote this as $X=(A_{1},A_{2},A_{3})$. Because they are binary, each of $A_{1},A_{2}$ and $A_{3}$ has possible outcomes 0 or 1. We’ll use the symbol $a_{1}$ for the outcome for $A_{1}$, $a_{2}$ for the outcome of $A_{2}$, and $a_{3}$ for the outcome of $A_{3}$. This means $a_{1},a_{2},a_{3} \\in \\{0,1\\}$.\n",
    "\n",
    "We can write outcomes, $x$, for the overall random variable $X$ as a three-digit bit-string, e.g. $x=010$ to represent the outcome $a_{1}=0,a_{2}=1,a_{3}=0$. There are $2^{3}=8$ possible values for $x$; these are $000,001,010,011,100,101,110,111$. The true probability distribution $P_{X}(x)$ is the same as $P_{A_{1},A_{2},A_{3}}(a_{1},a_{2},a_{3})$, and corresponds to 8 numbers (between 0 and 1) that all add up to 1 (because this is a probability distribution). We will use the notation $P_{X}$ and $P_{A_{1},A_{2},A_{3}}$ interchangeably.\n",
    "\n",
    "We now introduce our approximation $Q_{X}$. We’ll use a product approximation, so we write,\n",
    "$$\n",
    "Q_{A_{1},A_{2},A_{3}}(a_{1},a_{2},a_{3})= P_{A_{1}}^{(\"approx\")}(a_{1})\\,P_{A_{2}}^{(\"approx\")}(a_{2})\\, P_{A_{3}}^{(\"approx\" }(a_{3})\n",
    "$$\n",
    "\n",
    "We’ve put the superscript \"approx\" on the distributions in the right-hand side above to emphasize that we’re constructing an approximation and that $P_{A_{1}}^{(\"approx\")}(a_{1})$ is not the true marginal distribution $P_{A_{1}}(a_{1})$, but an approximation to it.\n",
    "\n",
    "### Q1. \n",
    "What is the only form the approximation $P_{A_{1}}^{(\"approx\")}(a_{1})$ can take? What are the parameters of this approximation?\n",
    "\n",
    "### Q2. \n",
    "Using this approximation for $P_{A_{1}}^{(\"approx\")}(a_{1})$, and similar approximations for $P_{A_{2}}^{(\"approx\")}(a_{2})$ and $P_{A_{3}}^{(\"approx\")}(a_{3})$, substitute these into the approximation fof $Q_{A_{1},A_{2},A_{3}}(a_{1},a_{2},a_{3})$ above to write down the full mathematical form for the approximation $Q_{A_{1},A_{2},A_{3}}(a_{1},a_{2},a_{3})$.\n",
    "\n",
    "### Q3.\n",
    "Using the mathematical expression for $Q_{A_{1},A_{2},A_{3}}(a_{1},a_{2},a_{3})$ that you wrote in Q2, derive an expression for the KL-divergence $D_{KL}\\left ( P_{A_{1},A_{2},A_{3}} || Q_{A_{1},A_{2},A_{3}} \\right )$, in terms of the parameters of $Q_{A_{1},A_{2},A_{3}}(a_{1},a_{2},a_{3})$ and the true marginal distributions $P_{A_{1}}(a_{1})$, $P_{A_{2}}(a_{2})$ and $P_{A_{3}}(a_{3})$.\n",
    "\n",
    "### Q4.\n",
    "Minimize the expression for the KL-divergence in Q3 with respect to the parameters of $Q_{A_{1},A_{2},A_{3}}(a_{1},a_{2},a_{3})$. Comment on the solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78d6c0a",
   "metadata": {},
   "source": [
    "# Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70d75a6",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "Since $A_1$ only has two possible outcomes, it is a Bernoulli random variable and there is only one mathematical form $P_{A_{1}}^{(approx)}(a_{1})$ can take. It is, $P_{A_{1}}^{(approx)}(a_{1}) = \\left ( p_{1}\\right )^{a_{1}}(1-p_{1})^{1-a_{1}}$.\n",
    "\n",
    "The number $p_{1}$ is between 0 and 1, and we use it as the success probability for the Bernoulli random variable $A_{1}$. It is a parameter we are free to adjust to make our overall approximation $Q_{X}$ as accurate as possible. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a11ca4c",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "If we use the same approach for $P_{A_{2}}^{(approx)}$ and $P_{A_{2}}^{(approx)}$ our overall approximation is,\n",
    "\n",
    "$Q_{X}(a_{1},a_{2},a_{3})= \\left ( p_{1} \\right )^{a_{1}}(1-p_{1})^{1-a_{1}}\\,\\left ( p_{2} \\right )^{a_{2}}(1-p_{2})^{1-a_{2}}\\,\\left ( p_{3} \\right )^{a_{3}}(1-p_{3})^{1-a_{3}}$\n",
    "\n",
    "\n",
    "Our approximation above has three parameters, $p_{1},p_{2},p_{3}$, that we can adjust to make $Q_{X}$ as close possible to the distribution $P_{X}$ which consists of 8 numbers. We adjust $p_{1},p_{2},p_{3}$ by minimizing $D_{KL}\\left ( P_{X} ||Q_{X} \\right )$ with respect to $p_{1},p_{2},p_{3}$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce1fb87",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "We can simplify the calculation by first re-writing $D_{KL} \\left ( P_{X} || Q_{X} \\right )$ as,\n",
    "\n",
    "$$\n",
    "D_{KL} \\left ( P_{X} || Q_{X} \\right )\\;=\\; -\\sum_{x} P_{X}(x)\\log⁡ Q_{X}(x)\\;+\\; \\sum_{x} P_{X}(x)\\log⁡ P_{X}(x)\n",
    "$$\n",
    "\n",
    "The second sum on the right-hand side of above only depends on $P_{X}$ so doesn’t depend on any of our parameters $p_{1},p_{2},p_{3}$. To determine the optimal values of $p_{1},p_{2},p_{3}$ we only need minimize the first sum on the right-hand side above. The second sum on the right-hand side above is actually $-H(X)$.\n",
    "\n",
    "Explicitly replacing $P_{X}$ with $P_{X}(a_{1}, a_{2}, a_{3})$ and $Q_{X}$ with the approximation in Q2, we get,\n",
    "\n",
    "$$ \n",
    "-\\sum_{x} P_{X}(x)\\log⁡ Q_{X}(x)\\;=\\; -\\sum_{\\left ( a_{1}, a_{2}, a_{3}\\right ) \\in \\{0,1\\}^{3}} P_{X}(a_{1},a_{2},a_{3}) \\left [ a_{1}\\log p_{1} \\;+ (1-a_{1})\\log (1-p_{1}) \\;+\\; a_{1}\\log p_{1} \\;+ (1-a_{2})\\log (1-p_{2}) \\;+\\; a_{3}\\log p_{3} \\;+ (1-a_{3})\\log (1-p_{3}) \\right ]\\newline\n",
    "\\;=\\; -\\sum_{\\left ( a_{1}, a_{2}, a_{3}\\right ) \\in \\{0,1\\}^{3}} P_{X}(a_{1},a_{2},a_{3}) \\left [ a_{1}\\log p_{1} \\;+ (1-a_{1})\\log (1-p_{1}) \\right ] \\newline\\;\n",
    "\\;-\\; \\sum_{\\left ( a_{1}, a_{2}, a_{3}\\right ) \\in \\{0,1\\}^{3}} P_{X}(a_{1},a_{2},a_{3}) \\left [ a_{2}\\log p_{2} \\;+ (1-a_{2})\\log (1-p_{2}) \\right ]\\newline \\;\n",
    "\\;-\\; \\sum_{\\left ( a_{1}, a_{2}, a_{3}\\right ) \\in \\{0,1\\}^{3}} P_{X}(a_{1},a_{2},a_{3}) \\left [ a_{3}\\log p_{3} \\;+ (1-a_{3})\\log (1-p_{3}) \\right ]\n",
    "$$\n",
    "\n",
    "Each of the summation on the right-hand side above only depends upon one of $a_{1}, a_{2}$ or $a_{3}$. If we recall the definition of the marginal distribution we have,\n",
    "\n",
    "$$\n",
    "\\sum_{\\left ( a_{2}, a_{3}\\right ) \\in \\{0,1\\}^{2}} P_{X}(a_{1},a_{2},a_{3})\\;=\\;P_{A_{1}}(a_{1})\n",
    "$$\n",
    "\n",
    "So we can write \n",
    "\n",
    "$$\n",
    "-\\sum_{x} P_{X}(x)\\log⁡ Q_{X}(x)\\;=\\; -\\sum_{a_{1}\\in \\{0,1\\}} P_{A_{1}}(a_{1}) \\left [ a_{1}\\log p_{1} \\;+ (1-a_{1})\\log (1-p_{1}) \\right ] \\newline \n",
    "\\;-\\; \\sum_{a_{2}\\in \\{0,1\\}} P_{A_{2}}(a_{2}) \\left [ a_{2}\\log p_{2} \\;+ (1-a_{2})\\log (1-p_{2}) \\right ]\\newline\n",
    "\\;-\\;\\sum_{a_{3}\\in \\{0,1\\}} P_{A_{3}}(a_{3}) \\left [ a_{3}\\log p_{1} \\;+ (1-a_{3})\\log (1-p_{3}) \\right ]\n",
    "$$\n",
    "\n",
    "This means we only need to evaluate the expressions like,\n",
    "$$\n",
    "\\sum_{a_{1}\\in \\{0,1\\}} P_{A_{1}}(a_{1})\\;=\\; 1\\;\\;,\\;\\;\n",
    "\\sum_{a_{1}\\in \\{0,1\\}} P_{A_{1}}(a_{1}) a_{1}\\;=\\;\\mathbb{E}\\left ( A_{1}\\right )\n",
    "$$\n",
    "\n",
    "with similar expressions involving $A_{2}$ and $A_{3}$. Here $\\mathbb{E}\\left ( A_{1}\\right )$ is the expectation value of the random variable $A_{1}$, i.e. the average over the true distribution $P_{A_{1}}(a_{1})$, which is a number between 0 and 1. \n",
    "\n",
    "Plugging in these expressions we get,\n",
    "\n",
    "$$\n",
    "-\\sum_{x} P_{X}(x)\\log⁡ Q_{X}(x)\\; =\\; \\left [ \\mathbb{E} \\left ( A_{1}\\right ) -1 \\right ]\\log \\left ( 1-p_{1}\\right ) \\;-\\; \\mathbb{E} \\left ( A_{1}\\right )\\log p_{1}\\;+\\; \\left [ \\mathbb{E} \\left ( A_{2}\\right ) - 1 \\right ]\\log \\left ( 1-p_{2}\\right ) \\;-\\; \\mathbb{E} \\left ( A_{2}\\right )\\log p_{2}\\newline\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\;+\\;\\left [ \\mathbb{E} \\left ( A_{3}\\right ) - 1 \\right ]\\log \\left ( 1-p_{3}\\right ) \\;-\\; \\mathbb{E} \\left ( A_{3}\\right )\\log p_{3}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077f8e94",
   "metadata": {},
   "source": [
    "## Q4\n",
    "\n",
    "We minimize our final expression in Q3 with respect to $p_{1}, p_{2}$ and $p_{2}$. We do this by taking the partial derivatives with respect to $p_{1}, p_{2}$ and $p_{3}$ and setting those partial derivatives to zero. Doing so we get,\n",
    "\n",
    "$$\n",
    "-\\frac{\\partial }{\\partial p_{1}} \\sum_{x}P_{X}(x)\\log Q_{X}(x)\\;=\\;0\\;=\\;\\frac{p_{1} - \\mathbb{E}\\left ( A_{1}\\right )}{p_{1}(1-p_{1})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "-\\frac{\\partial }{\\partial p_{2}} \\sum_{x}P_{X}(x)\\log Q_{X}(x)\\;=\\;0\\;=\\;\\frac{p_{2} - \\mathbb{E}\\left ( A_{2}\\right )}{p_{2}(1-p_{2})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "-\\frac{\\partial }{\\partial p_{3}} \\sum_{x}P_{X}(x)\\log Q_{X}(x)\\;=\\;0\\;=\\;\\frac{p_{3} - \\mathbb{E}\\left ( A_{3}\\right )}{p_{3}(1-p_{3})}\n",
    "$$\n",
    "\n",
    "Solving these equations gives,\n",
    "\n",
    "$$\n",
    "p_{1}\\;=\\;\\mathbb{E} \\left ( A_{1}\\right )\\;\\;,\\;\\; p_{2}\\;=\\;\\mathbb{E} \\left ( A_{3}\\right )\\;\\;,\\;\\; p_{3}\\;=\\;\\mathbb{E} \\left ( A_{3}\\right )\n",
    "$$\n",
    "\n",
    "We can check that this solution is indeed a minimum of the KL divergence by evaluating the second partial derivatives of $-\\sum_{x}P_{X}(x)\\log Q_{X}(x)$ at the solution point. Doing this we find,\n",
    "\n",
    "$$\n",
    "\\left . -\\frac{\\partial }{\\partial p_{1}} \\sum_{x}P_{X}(x)\\log Q_{X}(x)\\right |_{p_{1}=\\mathbb{E}\\left ( A_{1}\\right )}\\;=\\;\n",
    "\\frac{1}{1 - \\mathbb{E}\\left ( A_{1}\\right )}\\;>\\; 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\left . -\\frac{\\partial }{\\partial p_{2}} \\sum_{x}P_{X}(x)\\log Q_{X}(x)\\right |_{p_{2}=\\mathbb{E}\\left ( A_{2}\\right )}\\;=\\;\n",
    "\\frac{1}{1 - \\mathbb{E}\\left ( A_{2}\\right )}\\;>\\; 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\left . -\\frac{\\partial }{\\partial p_{3}} \\sum_{x}P_{X}(x)\\log Q_{X}(x)\\right |_{p_{3}=\\mathbb{E}\\left ( A_{3}\\right )}\\;=\\;\n",
    "\\frac{1}{1 - \\mathbb{E}\\left ( A_{3}\\right )}\\;>\\; 0\n",
    "$$\n",
    "\n",
    "So yes the solution point is a minimum of the KL-divergence. This means our optimal approximation is given by setting the paramaters $p_{1},p_{2},p_{3}$ equal to the expectation values $\\mathbb{E}\\left ( A_{1}\\right ), \\mathbb{E}\\left ( A_{2}\\right ), \\mathbb{E}\\left ( A_{2}\\right )$, respectively. \n",
    "\n",
    "Again, we probably could have anticipated this result, as it is very intuitive. The parameter $p_{1}$ is just the expectation of $A_{1}$ evaluated using the approximate distribution $Q_{X}$, so the optimal solution is just saying that we set the expectation of the approximate distribution equal to the expectation of the true distribution - a very intuitive result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
